<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>文生图 on 古月月仔的博客</title>
    <link>http://localhost:1313/tags/%E6%96%87%E7%94%9F%E5%9B%BE/</link>
    <description>Recent content in 文生图 on 古月月仔的博客</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 04 Nov 2025 00:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/%E6%96%87%E7%94%9F%E5%9B%BE/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>【论文阅读】ViewDiff：基于文生图模型的3D一致图像生成技术</title>
      <link>http://localhost:1313/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBviewdiff%E5%9F%BA%E4%BA%8E%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%9A%843d%E4%B8%80%E8%87%B4%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF/</link>
      <pubDate>Tue, 04 Nov 2025 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBviewdiff%E5%9F%BA%E4%BA%8E%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%9A%843d%E4%B8%80%E8%87%B4%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF/</guid>
      <description>&lt;p&gt;CVPR 2024论文ViewDiff提出了一种新方法，利用预训练的文生图扩散模型生成高质量且多视角一致的3D图像，旨在克服现有方法在真实感和视角一致性上的不足。&lt;/p&gt;&#xA;&lt;p&gt;大家好，我今天要分享的内容是ViewDiff 基于文生图模型的3D一致图像生成技术，是CVPR2024的一篇论文。&lt;/p&gt;&#xA;&lt;p&gt;论文原址：https://lukashoel.github.io/ViewDiff/&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBviewdiff%E5%9F%BA%E4%BA%8E%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%9A%843d%E4%B8%80%E8%87%B4%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF/image-20251104112946823-1762226997192-1.png&#34; &#xA;       alt=&#34;image-20251104112946823&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      image-20251104112946823&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;&#xA;&lt;p&gt;近年来 生成式人工智能(artificial intelligence generated content, AIGC)重新定义了视觉内容的生成、制作和编辑过程, 特别是自从扩散模型LDM（Latent Diffusion Model）和DiT (Diffusion Transformer)架构发布以来，由这两个架构所衍生的Stable Diffusion社区生态带来了AIGC领域的一片繁荣。&lt;/p&gt;&#xA;&lt;p&gt;最近国内常提起的豆包修图和Nannobanana也是基于上述的DIT框架。&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBviewdiff%E5%9F%BA%E4%BA%8E%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E7%9A%843d%E4%B8%80%E8%87%B4%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF/image-20251104112253545.png&#34; &#xA;       alt=&#34;image-20251104112253545&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      image-20251104112253545&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;&#xA;&lt;h3 id=&#34;过往方法&#34;&gt;过往方法&lt;/h3&gt;&#xA;&lt;p&gt;上述生成的内容主要局限在2D领域，扩散模型社区的繁荣吸引了很多研究人员探索扩散模型在3D内容生成领域的应用。&lt;/p&gt;&#xA;&lt;p&gt;现有方法普遍存在以下问题：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;优化类方法（如DreamFusion）生成结果非逼真、缺乏背景&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;相比起训练模型的2D数据,真实3D多视图数据的获取难度大，而通过2D数据合成的3D数据训练模型的模型普遍缺乏真实感。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;基于上述问题，本论文提出方法的核心目标是：&lt;/p&gt;&#xA;&lt;p&gt;利用预训练T2I模型先验，从真实数据中生成多视角一致的高质量图像&lt;/p&gt;&#xA;&lt;h3 id=&#34;前置知识&#34;&gt;前置知识&lt;/h3&gt;&#xA;&lt;p&gt;在讲述本文方法的实现细节之前，先来讲述一下前置工作&lt;/p&gt;</description>
    </item>
    <item>
      <title>【AIGC】生图模型概念&amp;模块宗述</title>
      <link>http://localhost:1313/posts/aigc%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9D%97%E5%AE%97%E8%BF%B0/</link>
      <pubDate>Wed, 22 Oct 2025 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/aigc%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9D%97%E5%AE%97%E8%BF%B0/</guid>
      <description>&lt;p&gt;文章概述了生成式AI在视觉内容领域的演进，从早期算法到VAE和GAN，再到当前主流的扩散模型，分析了其核心思想与特点，并指出AIGC正向更高分辨率、更丰富内容和更可控的方向发展。&lt;/p&gt;&#xA;&lt;p&gt;随着人工智能以空前的速度不断发展, 生成式人工智能(artificial intelligence generated content,  AIGC)重新定义了视觉内容的生成、制作和编辑过程, 彻底改变了我们感知和创造视觉内容的方式.&lt;/p&gt;&#xA;&lt;p&gt;早期算法通常使用图像匹配或人工设计的生成规则合成简单的纹理或结构; 随后, 深度学习时代提出变分自编码(variational auto-encoder,  VAE)和生成对抗网络(generative adversarial net work, GAN), 它们能够学习如人像、室内场景等典型的图像分布; 当前,涌现出了大型的&lt;strong&gt;扩散模型&lt;/strong&gt;来 生成图像和更具挑战性的视频, 其生成的质量之高 甚至是人也难以区分真假. 从传统算法到 VAE 和 GAN, 再到扩散模型, AIGC 正在向更高分辨率、更丰富内容和更可控的生成方向演变.&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/aigc%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9D%97%E5%AE%97%E8%BF%B0/%E3%80%90aigc%E3%80%91%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9D%97%E5%AE%97%E8%BF%B0/image-20251022230653074.png&#34; &#xA;       alt=&#34;图像生成模型架构和图像生成数据集的发展概览 &#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      图像生成模型架构和图像生成数据集的发展概览 &#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;图像生成模型&#34;&gt;图像生成模型&lt;/h2&gt;&#xA;&lt;p&gt;目前主流的图像生成模型大致可以分为以下四类：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;模型类型&lt;/th&gt;&#xA;          &lt;th&gt;代表模型&lt;/th&gt;&#xA;          &lt;th&gt;核心思想&lt;/th&gt;&#xA;          &lt;th&gt;特点&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;GAN（生成对抗网络）&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;StyleGAN、BigGAN、CycleGAN&lt;/td&gt;&#xA;          &lt;td&gt;生成器与判别器对抗训练&lt;/td&gt;&#xA;          &lt;td&gt;图像质量高，但训练不稳定，控制性差&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;VAE（变分自编码器）&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;VQ-VAE、DALL-E 1&lt;/td&gt;&#xA;          &lt;td&gt;编码为潜在变量再解码&lt;/td&gt;&#xA;          &lt;td&gt;稳定性好，但图像模糊，细节差&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Diffusion（扩散模型）&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Stable Diffusion、DALL-E 2/3、Imagen&lt;/td&gt;&#xA;          &lt;td&gt;去噪过程生成图像&lt;/td&gt;&#xA;          &lt;td&gt;图像质量高，控制性强，训练成本高&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;自回归模型&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Parti、ImageGPT&lt;/td&gt;&#xA;          &lt;td&gt;像语言模型一样逐像素/块生成&lt;/td&gt;&#xA;          &lt;td&gt;可扩展性强，但生成速度慢&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/aigc%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9D%97%E5%AE%97%E8%BF%B0/%E3%80%90aigc%E3%80%91%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9D%97%E5%AE%97%E8%BF%B0/image-20251022230951682.png&#34; &#xA;       alt=&#34;图像生成与编辑模型分类&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      图像生成与编辑模型分类&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
