<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on 古月月仔的博客</title>
    <link>http://localhost:1313/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 古月月仔的博客</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 30 Jan 2025 00:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>【深度学习】常见的激活函数</title>
      <link>http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</link>
      <pubDate>Thu, 30 Jan 2025 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</guid>
      <description>&lt;p&gt;激活函数为神经网络引入非线性，使其能够学习复杂模式。它分为饱和与非饱和两类，前者如Sigmoid和tanh，后者如ReLU。使用激活函数可避免网络退化为线性模型，并解决梯度消失等问题，从而增强模型的表达能力。&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;激活函数（Activation Function）&lt;/strong&gt;&lt;/em&gt;,是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/d9bc438935856aa36e7c169fb27dffbd.png&#34; &#xA;       alt=&#34;img&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      img&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;什么是激活函数&#34;&gt;什么是激活函数&lt;/h3&gt;&#xA;&lt;p&gt;在神经元中，输入的input经过一系列加权求和后作用于另一个函数，这个函数就是这里的激活函数。类似于人类大脑中基于神经元的模型，激活函数最终决定了是否传递信号以及要发射给下一个神经元的内容。&#xA;激活函数为神经网络引入了非线性元素，使得网络能够逼近复杂的非线性函数，从而解决更广泛的问题。&lt;/p&gt;&#xA;&lt;h3 id=&#34;为什么要使用激活函数&#34;&gt;为什么要使用激活函数&lt;/h3&gt;&#xA;&lt;p&gt;如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的&lt;em&gt;&lt;strong&gt;感知机（Perceptron）&lt;/strong&gt;&lt;/em&gt;。&#xA;使用激活函数能够给神经元引入非线性因素，使得神经网络可以任意逼近任何非线性函数，使深层神经网络表达能力更加强大，这样神经网络就可以应用到众多的非线性模型中。&lt;/p&gt;&#xA;&lt;h3 id=&#34;激活函数的分类&#34;&gt;激活函数的分类&lt;/h3&gt;&#xA;&lt;p&gt;基于激活函数的本质，可以将激活函数分为以下两大类：&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image-20250425155707483.png&#34; &#xA;       alt=&#34;image-20250425155707483&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      image-20250425155707483&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;饱和激活函数&lt;/strong&gt;&lt;/em&gt;：Sigmoid、tanh&amp;hellip;&amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;非饱和激活函数&lt;/strong&gt;&lt;/em&gt;：ReLU、LeakyRelu、ELU、PReLU、RReLU&amp;hellip;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;什么是饱和&#34;&gt;什么是饱和？&lt;/h4&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image-20250425155722212.png&#34; &#xA;       alt=&#34;image-20250425155722212&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      image-20250425155722212&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;反之，不满足以上条件的函数则称为非饱和激活函数。&#xA;&lt;em&gt;&lt;strong&gt;Sigmoid&lt;/strong&gt;&lt;/em&gt;函数需要一个实值输入压缩至[0,1]的范围&#xA;&lt;em&gt;&lt;strong&gt;tanh函数&lt;/strong&gt;&lt;/em&gt;需要讲一个实值输入压缩至 [-1, 1]的范围&#xA;相对于饱和激活函数，使用非饱和激活函数的优势在于两点：&lt;/p&gt;</description>
    </item>
    <item>
      <title>【机器学习】梯度下降与损失函数</title>
      <link>http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</link>
      <pubDate>Mon, 20 Jan 2025 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</guid>
      <description>&lt;p&gt;梯度下降是一种通过计算目标函数梯度并沿其反方向迭代调整参数以寻找最小值的优化方法。它利用导数、偏导数和方向导数等概念，在机器学习中常用于最小化损失函数，但可能收敛于局部最优解而非全局最优。&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;梯度下降（Gradient Descent GD）&lt;em&gt;&lt;strong&gt;简单来说就是一种寻找&lt;/strong&gt;&lt;/em&gt;目标函数&lt;/strong&gt;&lt;/em&gt;最小化的方法，它利用梯度信息，通过不断迭代调整参数来寻找合适的目标值。&lt;/p&gt;&#xA;&lt;h3 id=&#34;什么是梯度&#34;&gt;什么是梯度？&lt;/h3&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/v2-6d78a2ab1092a44151213358e805bace_1440w.png&#34; &#xA;       alt=&#34;Gradient-Descent（全世界最通俗易懂的梯度下降法详解-优化函数大法）&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      Gradient-Descent（全世界最通俗易懂的梯度下降法详解-优化函数大法）&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;p&gt;关于梯度的引入，可以分为四个概念：导数-&amp;gt;偏导-&amp;gt;方向导数-&amp;gt;梯度&#xA;&lt;em&gt;&lt;strong&gt;导数&lt;/strong&gt;&lt;/em&gt;：当函数定义域和取值都在实数域中时，导数可以表示函数曲线上的切线斜率。&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/20160325131532476.png&#34; &#xA;       alt=&#34;经典的导数与微分示意图&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      经典的导数与微分示意图&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;偏导数&lt;/strong&gt;&lt;/em&gt;：偏导其实就是多元函数一个多变量的函数的偏导数是它关于其中一个变量的导数，而保持其他变量恒定。因为曲面上的每一点都有无穷多条切线，描述这种函数的导数相当困难。偏导数就是选择其中一条切线，并求出它的斜率 。几何意义是表示固定面上一点的切线斜率。&#xA;多元函数降维时候的变化，比如[二元函数]固定y，只让x单独变化，从而看成是关于x的[一元函数]的变化来研究。&#xA;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/70.jpeg&#34; &#xA;       alt=&#34;这里写图片描述&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      这里写图片描述&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;但是偏导数有一个缺点，就是只能表示多元函数沿[坐标轴]方向的变化率，但是很多时候要考虑多元函数沿任意方向的变化率，于是就有了方向导数。&#xA;&lt;em&gt;&lt;strong&gt;方向导数&lt;/strong&gt;&lt;/em&gt;：某个方向的导数，本质就是函数在A点上无数个切线的[斜率]的定义，每个切线都代表一个方向，每个方向都是有方向导数的。&lt;/p&gt;</description>
    </item>
    <item>
      <title>卷积神经网络(CNN)的原理</title>
      <link>http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn%E7%9A%84%E8%AF%A6%E7%BB%86%E5%8E%9F%E7%90%86/</link>
      <pubDate>Sun, 10 Nov 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn%E7%9A%84%E8%AF%A6%E7%BB%86%E5%8E%9F%E7%90%86/</guid>
      <description>&lt;p&gt;卷积神经网络（CNN）的核心运算“卷积”是一种数学操作，通过翻转和滑动叠加来捕捉数据的局部与全局特征。这种机制使CNN在图像识别等计算机视觉任务中表现出色，能够有效处理图像中的平移不变性。&lt;/p&gt;&#xA;&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;&#xA;&lt;p&gt;卷积神经网络（Convolutional Neural Network，CNN）是一种在计算机视觉领域取得了巨大成功的深度学习模型。它们的设计灵感来自于生物学中的视觉系统，旨在模拟人类视觉处理的方式。在过去的几年中，CNN已经在图像识别、目标检测、图像生成和许多其他领域取得了显著的进展，成为了计算机视觉和深度学习研究的重要组成部分。&lt;/p&gt;&#xA;&lt;h2 id=&#34;卷积是什么&#34;&gt;卷积是什么？&lt;/h2&gt;&#xA;&lt;p&gt;教科书上一般定义函数f,g的卷积f*g(n)如下：&#xA;连续形式：&#xA;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn%E7%9A%84%E8%AF%A6%E7%BB%86%E5%8E%9F%E7%90%86/9c2f215a5a4e799b471a83ecbc95044e.jpeg&#34; &#xA;       alt=&#34;在这里插入图片描述&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      在这里插入图片描述&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;离散形式：&#xA;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn%E7%9A%84%E8%AF%A6%E7%BB%86%E5%8E%9F%E7%90%86/237b89f84f60bc0183a2cde78336a38f.jpeg&#34; &#xA;       alt=&#34;在这里插入图片描述&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      在这里插入图片描述&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;并且也解释了，先对g函数进行翻转，相当于在数轴上把g函数从右边褶到左边去，也就是卷积的“卷”的由来。&#xA;然后再把g函数平移到n，在这个位置对两个函数的对应点相乘，然后相加，这个过程是卷积的“积”的过程。&lt;/p&gt;&#xA;&lt;h4 id=&#34;对卷积的通俗理解&#34;&gt;对卷积的通俗理解&lt;/h4&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加&lt;/strong&gt;&lt;/em&gt;&#xA;在连续情况下，叠加指的是对两个函数的乘积求积分，在离散情况下就是加权求和，为简单起见就统一称为叠加。&#xA;整体看来是这么个过程：&#xA;翻转——&amp;gt;滑动——&amp;gt;叠加——&amp;gt;滑动——&amp;gt;叠加——&amp;gt;滑动——&amp;gt;叠加 &amp;hellip;.&#xA;多次滑动得到的一系列叠加值，构成了卷积函数。&#xA;&lt;em&gt;&lt;strong&gt;卷积的“卷”&lt;/strong&gt;&lt;/em&gt;，指的的函数的翻转，从 g(t) 变成 g(-t) 的这个过程；&#xA;&lt;em&gt;&lt;strong&gt;卷积的“积”&lt;/strong&gt;&lt;/em&gt;，指的是滑动积分/加权求和。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;  &lt;blockquote&gt;&#xA;    &lt;p&gt;对卷积的意义的理解：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;从“积”的过程可以看到，我们得到的叠加值，是个全局的概念。以信号分析为例，卷积的结果是不仅跟当前时刻输入信号的响应值有关，也跟过去所有时刻输入信号的响应都有关系，考虑了对过去的所有输入的效果的累积。在图像处理的中，卷积处理的结果，其实就是把每个像素周边的，甚至是整个图像的像素都考虑进来，对当前像素进行某种加权处理。所以说，“积”是全局概念，或者说是一种“混合”，把两个函数在时间或者空间上进行混合。&lt;/li&gt;&#xA;&lt;li&gt;那为什么要进行“卷”？直接相乘不好吗？我的理解，进行“卷”（翻转）的目的其实是施加一种约束，它指定了在“积”的时候以什么为参照。在信号分析的场景，它指定了在哪个特定时间点的前后进行“积”，在空间分析的场景，它指定了在哪个位置的周边进行累积处理。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;  &lt;/blockquote&gt;&#xA;&#xA;&lt;h2 id=&#34;卷积神经网络&#34;&gt;卷积神经网络&lt;/h2&gt;&#xA;&lt;h3 id=&#34;图像原理&#34;&gt;图像原理&lt;/h3&gt;&#xA;&lt;p&gt;在了解卷积神经网络前，我们先来看看图像的原理：&#xA;图像在计算机中是一堆按顺序排列的数字，数值为0到255。0表示最暗，255表示最亮。 如下图：&lt;/p&gt;</description>
    </item>
    <item>
      <title>【机器学习】基础知识总结</title>
      <link>http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/</link>
      <pubDate>Mon, 30 Oct 2023 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/</guid>
      <description>&lt;p&gt;本文为机器学习复习资料，适合已有基础的学习者快速回顾。内容涵盖机器学习的定义，并重点梳理了有监督学习、无监督学习、半监督学习和强化学习的主要算法类型及其典型应用场景。&lt;/p&gt;&#xA;&lt;p&gt;最近在复习专业课时，发现去年上过的&lt;strong&gt;机器学习&lt;/strong&gt;的内容几乎已经都还给老师了（悲~），再去刷一遍课程的时间成本太高，所以就结合我去年画的思维导图和期末复习总结的一些资料汇总复习一下，内容有些来自公众号，有些来自老学长的圣遗物，有些是CSDN上看的。由于时间太长无法一一标明出处了，如果有侵权记得私信提醒我哈。&lt;/p&gt;&#xA;&lt;p&gt;本文适合对机器学习已经有过学习和了解，准备期末突击/定时复习一下的同学，**如果你对机器学习尚无了解，推荐你先去看吴恩达老师的Machine Learning课程，一定会让你受益匪浅。**下附老师的课程地址(B站)。&lt;/p&gt;&#xA;&lt;p&gt;[高清重置版-吴恩达机器学习](【【高清重制】2025年公认最好的【吴恩达机器学习课程】附课件、代码及实战项目！！！&amp;ndash;人工智能/机器学习/深度学习】 &lt;a href=&#34;https://www.bilibili.com/video/BV1owrpYKEtP/?p=145&amp;amp;share_source=copy_web&amp;amp;vd_source=a06df7b174b0e55e45242729b8ce1758&#34; target=&#34;_blank&#34;&gt;https://www.bilibili.com/video/BV1owrpYKEtP/?p=145&amp;share_source=copy_web&amp;vd_source=a06df7b174b0e55e45242729b8ce1758&lt;/a&gt;&#xA;)&lt;/p&gt;&#xA;&lt;h2 id=&#34;一什么是机器学习&#34;&gt;一、什么是机器学习？&lt;/h2&gt;&#xA;&lt;p&gt;百度解释是：机器学习是人工智能的一个分支，它使计算机系统能够从数据中学习并做出决策或预测，而无需进行明确的编程。简单来说，机器学习涉及到开发算法和统计模型，这些模型可以对输入数据进行分析，以预测结果或行为。&lt;/p&gt;&#xA;&lt;p&gt;讲人话就是&lt;strong&gt;现在有一个数据集，你要通过这个数据集找出一个函数/方法让分析数据，对数据做预测。所以机器学习=找function。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;二机器学习算法类型&#34;&gt;二、机器学习算法类型&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1有监督学习supervised-learning&#34;&gt;1.有监督学习（&lt;strong&gt;Supervised Learning&lt;/strong&gt;）&lt;/h3&gt;&#xA;&lt;p&gt;有监督学习通常是&lt;strong&gt;利用带有标签（tags）的训练数据（labeled data）&lt;/strong&gt;，学习一个从输入变量X到输入变量Y的函数映射。&lt;/p&gt;&#xA;&lt;p&gt;公式为：&lt;/p&gt;&#xA;&lt;p&gt;y=f(x)&lt;/p&gt;&#xA;&lt;p&gt;利用有监督学习解决的问题大致上可以被分为两类：&lt;/p&gt;&#xA;&lt;p&gt;**分类问题：预测某一样本所属的类别（离散的）。**比如给定一个人（从数据的角度来说，是给出一个人的数据结构，包括：身高，年龄，体重等信息），然后判断是性别，或者是否健康。&lt;/p&gt;&#xA;&lt;p&gt;**回归问题：预测某一样本的所对应的实数输出（连续的）。**比如预测某一地区人的平均身高。&lt;/p&gt;&#xA;&lt;p&gt;常见的有监督学习算法：线性回归，逻辑回归，分类回归树，朴素贝叶斯，K最近邻算法等。&lt;/p&gt;&#xA;&lt;p&gt;除此之外，集成学习也是一种有监督学习。它是将多个不同的相对较弱的机器学习模型的预测组合起来，用来预测新的样本。如随机森林装袋法，和XGBoost算法。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2无监督学习unsupervised-learning&#34;&gt;2.无监督学习(Unsupervised Learning)&lt;/h3&gt;&#xA;&lt;p&gt; 无监督学习问题处理的是&lt;strong&gt;只有输入变量X没有相应输出变量的训练数据。它利用没有专家标注训练数据(unlabeled data)&lt;/strong&gt;，对数据的结构建模。&lt;/p&gt;&#xA;&lt;p&gt;可以利用无监督学习解决的问题，大致分为四类：&lt;/p&gt;&#xA;&lt;p&gt;**关联分析：发现不同事物之间同时出现的概率。**在购物篮分析中被广泛地应用。比如经典的发现买牛奶的男客户有百分之八十的概率买啤酒。&lt;/p&gt;&#xA;&lt;p&gt;**聚类问题：将相似的样本划分为一个簇（cluster）。**与分类问题不同，聚类问题预先并不知道类别，自然训练数据也没有类别的标签。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;维度约减(数据降维)&lt;/strong&gt;：顾名思义，**维度约减是指减少数据的维度同时保证不丢失有意义的信息。**利用特征提取方法和特征选择方法，可以达到维度约减的效果。特征选择是指选择原始变量的子集。特征提取是将数据从高纬度转换到低纬度。广为熟知的主成分分析算法就是特征提取的方法。&lt;/p&gt;&#xA;&lt;p&gt;**异常检测：**暂时想不出例子来，知道有这个就行&lt;/p&gt;&#xA;&lt;p&gt;Apriori算法，K-means算法，PCA主成分分析，都属于无监督学习。&lt;/p&gt;&#xA;&lt;h3 id=&#34;3-半监督学习semi-supervised-learning&#34;&gt;3. &lt;strong&gt;半监督学习（Semi-supervised Learning）&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;这是监督学习和无监督学习的结合，使用少量标记数据和大量未标记数据。&lt;/p&gt;&#xA;&lt;h3 id=&#34;4强化学习reinforcement-learning&#34;&gt;4.&lt;strong&gt;强化学习（Reinforcement Learning）&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;通过学习可以获得最大回报的行为，强化学习可以让agent（个体）根据自己当前的状态，来决定下一步采取的动作。&lt;/p&gt;&#xA;&lt;p&gt;强化学习算法通过反复试验来学习最优的动作。这类算法在机器人学中被广泛应用。在与障碍物碰撞后，机器人通过传感收到负面的反馈从而学会去避免碰撞。&lt;/p&gt;&#xA;&lt;h2 id=&#34;三常见概念和疑问&#34;&gt;三、常见概念和疑问&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1常见机器学习算法概念简介&#34;&gt;1.常见机器学习算法概念简介：&lt;/h3&gt;&#xA;&lt;p&gt;1、监督学习（SupervisedLearning）：有类别标签的学习，基于训练样本的输入、输出训练得到最优模型，再使用该模型预测新输入的输出；&lt;/p&gt;&#xA;&lt;p&gt;代表算法：决策树、朴素贝叶斯、逻辑回归、KNN、SVM、神经网络、随机森林、AdaBoost、遗传算法；&lt;/p&gt;&#xA;&lt;p&gt;2、半监督学习（Semi-supervisedLearning）：同时使用大量的未标记数据和标记数据，进行模式识别工作；&lt;/p&gt;&#xA;&lt;p&gt;代表算法：self-training(自训练算法)、generative models生成模型、SVMs半监督支持向量机、graph-basedmethods图论方法、 multiviewlearing多视角算法等；&lt;/p&gt;&#xA;&lt;p&gt;3、无监督学习（UnsupervisedLearning）：无类别标签的学习，只给定样本的输入，自动从中寻找潜在的类别规则；代表算法：主成分分析方法PCA等，等距映射方法、局部线性嵌入方法、拉普拉斯特征映射方法、黑塞局部线性嵌入方法、局部切空间排列方法等；&lt;/p&gt;&#xA;&lt;p&gt;4、HOG特征：全称Histogram of Oriented Gradient（方向梯度直方图），由图像的局部区域梯度方向直方图构成特征；&lt;/p&gt;&#xA;&lt;p&gt;5、LBP特征：全称Local Binary Pattern（局部二值模式），通过比较中心与邻域像素灰度值构成图像局部纹理特征；&lt;/p&gt;&#xA;&lt;p&gt;6、Haar特征：描述图像的灰度变化，由各模块的像素差值构成特征；&lt;/p&gt;&#xA;&lt;p&gt;7、核函数（Kernels）：从低维空间到高维空间的映射，把低维空间中线性不可分的两类点变成线性可分的；&lt;/p&gt;&#xA;&lt;p&gt;8、&lt;strong&gt;SVM：全称Support Vector Machine（支持向量机）&lt;/strong&gt; 在特征空间上找到最佳的超平面使训练集正负样本的间隔最大；是解决二分类问题的有监督学习算法，引入核方法后也可用来解决非线性问题；&lt;/p&gt;&#xA;&lt;p&gt;9、Adaboost：全称Adaptive Boosting（自适应增强），对同一个训练集训练不同的弱分类器，把这些弱分类器集合起来，构成一个更强的强分类器；&lt;/p&gt;&#xA;&lt;p&gt;10、&lt;strong&gt;决策树算法（Decision Tree）&lt;/strong&gt;：处理训练数据，构建决策树模型，再对新数据进行分类；&lt;/p&gt;&#xA;&lt;p&gt;11、随机森林算法（Random Forest）：使用基本单元（决策树），通过集成学习将多棵树集成；&lt;/p&gt;&#xA;&lt;p&gt;12、朴素贝叶斯（Naive Bayes）：根据事件的先验知识描述事件的概率，对联合概率建模来获得目标概率值；&lt;/p&gt;&#xA;&lt;p&gt;13、&lt;strong&gt;神经网络（Neural Networks）&lt;/strong&gt;：模仿动物神经网络行为特征，将许多个单一“神经元”联结在一起，通过调整内部大量节点之间相互连接的关系，进行分布式并行信息处理。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
