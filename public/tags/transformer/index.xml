<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer on 古月月仔的博客</title>
    <link>http://localhost:1313/tags/transformer/</link>
    <description>Recent content in Transformer on 古月月仔的博客</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 30 Sep 2025 00:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>【LLM技术】Transformer架构宗述</title>
      <link>http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/</link>
      <pubDate>Tue, 30 Sep 2025 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/</guid>
      <description>&lt;p&gt;NLP技术从传统统计方法演进至神经网络，Word Embeddings、RNN/LSTM等模型逐步发展。Transformer凭借自注意力机制实现突破，成为里程碑，并催生了BERT、GPT等变革性模型。&lt;/p&gt;&#xA;&lt;h3 id=&#34;博文背景&#34;&gt;博文背景&lt;/h3&gt;&#xA;&lt;p&gt;之前在学习大模型相关技术的时候，看了很多篇对Transfomer解释的博客，但是感觉总是理解不透彻，不久之后就忘掉了，打算自己整&lt;/p&gt;&#xA;&lt;p&gt;理输出一下，以便更好的理解。&lt;/p&gt;&#xA;&lt;p&gt;要讲Transformer,当然要先从NLP的技术应用演进开始讲起：&lt;/p&gt;&#xA;&lt;h2 id=&#34;nlp技术演进背景&#34;&gt;NLP技术演进背景&lt;/h2&gt;&#xA;&lt;h3 id=&#34;传统的nlp方法统计学习时期&#34;&gt;&lt;strong&gt;传统的NLP方法（统计学习时期）&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;在神经网络技术普及之前，NLP领域主要依赖于&lt;strong&gt;基于规则的方法&lt;/strong&gt;和&lt;strong&gt;统计学习方法&lt;/strong&gt;。这些方法通过人工构建语言模型和统计方法来处理文本。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;基于规则的系统&lt;/strong&gt;：依赖大量的语言学规则，如语法、句法规则和词汇规则，这些规则通常由语言学家手工编写。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;统计学习方法&lt;/strong&gt;：随着计算机能力的提升，统计方法开始兴起，常见的算法包括隐马尔可夫模型（HMM）、最大熵模型、条件随机场（CRF）等。这些方法通过学习大量标注数据中的统计规律来进行预测，比如词性标注、命名实体识别等。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;然而，这些方法有两个主要局限：第一，它们需要大量的手工标注数据；第二，基于规则的方法难以处理复杂的语言现象。&lt;/p&gt;&#xA;&lt;h3 id=&#34;神经网络的兴起&#34;&gt;&lt;strong&gt;神经网络的兴起&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;神经网络的引入标志着NLP技术的根本转折。最早的神经网络应用在NLP中并不是很成功，直到2010年代深度学习的突破，才迎来了飞速的发展。&lt;/p&gt;&#xA;&lt;h4 id=&#34;word-embeddings&#34;&gt;Word Embeddings&lt;/h4&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251015205211766.png&#34; &#xA;       alt=&#34;词嵌入技术示意&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      词嵌入技术示意&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;p&gt;在传统方法中，词语是通过稀疏的词袋模型（Bag of Words）来表示的，这种方法无法捕捉词与词之间的关系。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt;（2013年）和&lt;strong&gt;GloVe&lt;/strong&gt;（2014年）是深度学习在NLP中的第一个重大成功。它们通过神经网络学习将词语映射到一个连续的向量空间中，称为&lt;strong&gt;词嵌入（Word Embedding）&lt;/strong&gt;，使得词与词之间的关系可以通过向量空间的几何距离来反映。例如，“国王”和“王后”在词嵌入空间中的距离比“国王”和“汽车”要近，这种方式有效地捕捉了词的语义。&lt;/p&gt;&#xA;&lt;h4 id=&#34;rnn和lstm&#34;&gt;&lt;strong&gt;RNN和LSTM&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251015205426121.png&#34; &#xA;       alt=&#34;RNN机构示意&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      RNN机构示意&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;p&gt;**RNN（循环神经网络）**是处理序列数据（如文本）的核心架构之一，它可以通过循环连接记住前一时刻的信息。然而，RNN有一个缺陷——它难以捕捉长期依赖关系。为了解决这个问题，**LSTM（长短时记忆网络）**被提出（1997年），它能够更好地处理长期依赖问题。**GRU（门控循环单元）**是LSTM的一种变体，它简化了结构并且在某些任务中表现出与LSTM相当的效果。&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251015205527512.png&#34; &#xA;       alt=&#34;LSMT结构示意&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      LSMT结构示意&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
