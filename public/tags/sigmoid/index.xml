<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sigmoid on 古月月仔的博客</title>
    <link>http://localhost:1313/tags/sigmoid/</link>
    <description>Recent content in Sigmoid on 古月月仔的博客</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 30 Jan 2025 00:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/sigmoid/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>【深度学习】常见的激活函数</title>
      <link>http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</link>
      <pubDate>Thu, 30 Jan 2025 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</guid>
      <description>&lt;p&gt;激活函数为神经网络引入非线性，使其能够学习复杂模式。它分为饱和与非饱和两类，前者如Sigmoid和tanh，后者如ReLU。使用激活函数可避免网络退化为线性模型，并解决梯度消失等问题，从而增强模型的表达能力。&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;激活函数（Activation Function）&lt;/strong&gt;&lt;/em&gt;,是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/d9bc438935856aa36e7c169fb27dffbd.png&#34; &#xA;       alt=&#34;img&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      img&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;什么是激活函数&#34;&gt;什么是激活函数&lt;/h3&gt;&#xA;&lt;p&gt;在神经元中，输入的input经过一系列加权求和后作用于另一个函数，这个函数就是这里的激活函数。类似于人类大脑中基于神经元的模型，激活函数最终决定了是否传递信号以及要发射给下一个神经元的内容。&#xA;激活函数为神经网络引入了非线性元素，使得网络能够逼近复杂的非线性函数，从而解决更广泛的问题。&lt;/p&gt;&#xA;&lt;h3 id=&#34;为什么要使用激活函数&#34;&gt;为什么要使用激活函数&lt;/h3&gt;&#xA;&lt;p&gt;如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的&lt;em&gt;&lt;strong&gt;感知机（Perceptron）&lt;/strong&gt;&lt;/em&gt;。&#xA;使用激活函数能够给神经元引入非线性因素，使得神经网络可以任意逼近任何非线性函数，使深层神经网络表达能力更加强大，这样神经网络就可以应用到众多的非线性模型中。&lt;/p&gt;&#xA;&lt;h3 id=&#34;激活函数的分类&#34;&gt;激活函数的分类&lt;/h3&gt;&#xA;&lt;p&gt;基于激活函数的本质，可以将激活函数分为以下两大类：&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image-20250425155707483.png&#34; &#xA;       alt=&#34;image-20250425155707483&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      image-20250425155707483&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;饱和激活函数&lt;/strong&gt;&lt;/em&gt;：Sigmoid、tanh&amp;hellip;&amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;&lt;strong&gt;非饱和激活函数&lt;/strong&gt;&lt;/em&gt;：ReLU、LeakyRelu、ELU、PReLU、RReLU&amp;hellip;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;什么是饱和&#34;&gt;什么是饱和？&lt;/h4&gt;&#xA;&lt;p&gt;&lt;figure style=&#34;text-align: center; margin: 1.5rem auto;&#34;&gt;&#xA;  &#xA;  &lt;img src=&#34;http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image-20250425155722212.png&#34; &#xA;       alt=&#34;image-20250425155722212&#34; &#xA;       &#xA;       class=&#34;zoomable&#34; &#xA;       style=&#34;max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;&#34;&#xA;       loading=&#34;lazy&#34; /&gt;&#xA;  &#xA;    &lt;figcaption style=&#34;margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;&#34;&gt;&#xA;      image-20250425155722212&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;反之，不满足以上条件的函数则称为非饱和激活函数。&#xA;&lt;em&gt;&lt;strong&gt;Sigmoid&lt;/strong&gt;&lt;/em&gt;函数需要一个实值输入压缩至[0,1]的范围&#xA;&lt;em&gt;&lt;strong&gt;tanh函数&lt;/strong&gt;&lt;/em&gt;需要讲一个实值输入压缩至 [-1, 1]的范围&#xA;相对于饱和激活函数，使用非饱和激活函数的优势在于两点：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
