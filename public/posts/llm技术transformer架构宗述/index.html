<!DOCTYPE html>
<html lang="zh-cn"
  x-data
  :class="$store.darkMode.class()"
  :data-theme="$store.darkMode.theme()">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【LLM技术】Transformer架构宗述 | 古月月仔的博客</title>

    

<link rel="canonical" href="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/" />


<meta name="author" content="古月月仔" />
<meta name="description" content="NLP技术从传统统计方法演进至神经网络，Transformer凭借自注意力机制实现突破，成为里程碑，并催生了BERT、GPT等变革性模型。" />
<meta name="keywords" content="LLM,大模型,Transformer,NLP">


<meta name="generator" content="Hugo 0.153.1">

<meta property="og:url" content="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/">
  <meta property="og:site_name" content="古月月仔的博客">
  <meta property="og:title" content="【LLM技术】Transformer架构宗述">
  <meta property="og:description" content="NLP技术从传统统计方法演进至神经网络，Transformer凭借自注意力机制实现突破，成为里程碑，并催生了BERT、GPT等变革性模型。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-30T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-09-30T00:00:00+08:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="大模型">
    <meta property="article:tag" content="Transformer">
    <meta property="article:tag" content="NLP">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="【LLM技术】Transformer架构宗述">
  <meta name="twitter:description" content="NLP技术从传统统计方法演进至神经网络，Transformer凭借自注意力机制实现突破，成为里程碑，并催生了BERT、GPT等变革性模型。">




<link rel="stylesheet" href="/css/output.css" />



<script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3/dist/cdn.min.js"></script>

    


<style>
  pre {
    padding: 1em;
    overflow: auto;
  }
</style>









    

    <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3/dist/cdn.min.js" integrity="sha256-4EHxtjnR5rL8JzbY12OKQJr81ESm7JBEb49ORPo29AY=" crossorigin="anonymous"></script>
  </head>
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js"></script>

<style>
   
  .medium-zoom-overlay {
    z-index: 100;
    background: rgba(0, 0, 0, 0.8) !important;
  }
  .medium-zoom-image--opened {
    z-index: 101;
  }
</style>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    
    const zoom = mediumZoom('.zoomable', {
      margin: 24,          
      background: '#000',  
      scrollOffset: 40,    
    });
  });
</script>
  <body x-data="{
    flip: false,
  }">
    
    <div id="dream-global-bg"></div>

    
<nav class="mt-4 lg:mt-8 py-4">

  
  <div class="container flex justify-between px-4">
  
    <section class="flex items-center gap-4">
      <div class="avatar cursor-pointer hover:avatar-online" @click="flip = !flip" title="Flip it!">
        <div class="h-10 rounded-full">
          <img src="/img/ChipDog.jpg" alt="古月月仔的博客" />
        </div>
      </div>

      
      <div>
        
        <a href="http://localhost:1313/" class="text-lg font-semibold cursor-pointer">
          古月月仔的博客
        </a>
        
        
        <div class="text-base-content/60 text-sm">计算机在校生 || 游戏程序员
记录从基础到进阶的每一份思考。</div>
        
      </div>
      
    </section>

    
    

    <div class="dropdown dropdown-end sm:hidden">
      <div tabindex="0" role="button" class="btn btn-ghost btn-square" aria-label="Select an option">
        <ion-icon name="menu" class="text-2xl"></ion-icon>
      </div>
      <ul tabindex="0" class="dropdown-content menu w-36 bg-base-100 rounded-box z-1 shadow-md">
        







<li>
  <div role="link" tabindex="0" class="inline-flex items-center p-2 cursor-pointer" @click="flip = !flip" title="About">
    <ion-icon name="information-circle"></ion-icon>About</div>
</li>





















<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/posts" title="Archives">
    <ion-icon name="archive"></ion-icon>
    Archives
  </a>
</li>




<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/categories" title="All Categories">
    <ion-icon name="grid"></ion-icon>
    All Categories
  </a>
</li>




<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/tags" title="All Tags">
    <ion-icon name="pricetags"></ion-icon>
    All Tags
  </a>
</li>






      </ul>
    </div>
    <section class="hidden sm:flex sm:items-center sm:gap-2 md:gap-4">
      

      
      




<div role="link" tabindex="0" class="text-sm font-semibold cursor-pointer hover:underline" @click="flip = !flip" title="About">About</div>





      
      





      
      





      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/posts" title="Archives">
  <ion-icon class="group-hover:text-primary-content" name="archive"></ion-icon>
</a>


      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/categories" title="All Categories">
  <ion-icon class="group-hover:text-primary-content" name="grid"></ion-icon>
</a>


      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/tags" title="All Tags">
  <ion-icon class="group-hover:text-primary-content" name="pricetags"></ion-icon>
</a>


      

      

      
    </section>
  </div>
</nav>


    <div class="flip-container" :class="{ 'flip-it': flip }">
      <div class="flipper">
        <div class="front">
          <div class="container">
            
<div class="lg:grid lg:grid-cols-4 gap-4 mt-4 px-4">
  <div class="hidden lg:block">
    
  </div>

  <div class="lg:col-span-2">
    <article class="mx-auto prose prose-quoteless dark:prose-invert" id="dream-single-post-main" itemscope itemtype="http://schema.org/Article">
      
  <meta itemprop="name" content="【LLM技术】Transformer架构宗述">
  <meta itemprop="description" content="NLP技术从传统统计方法演进至神经网络，Transformer凭借自注意力机制实现突破，成为里程碑，并催生了BERT、GPT等变革性模型。">
  <meta itemprop="datePublished" content="2025-09-30T00:00:00+08:00">
  <meta itemprop="dateModified" content="2025-09-30T00:00:00+08:00">
  <meta itemprop="wordCount" content="565">
  <meta itemprop="keywords" content="LLM,大模型,Transformer,NLP">

      <header>
        <h1 itemprop="headline">【LLM技术】Transformer架构宗述</h1>
        <p class="text-sm">
          
            Tuesday, Sep 30, 2025
          

          | <span>3 minute read</span>

          
          | <span>Updated at
            
              Tuesday, Sep 30, 2025
            </span>
          
        </p>

        
        <div class="flex justify-between">
          
            <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name">古月月仔</span>
  
  </span>
</div>

          

          <div class="flex items-center gap-2">
  
  

  
  
  
  
  
    <a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary"
      href="https://x.com/intent/post?text=%e3%80%90LLM%e6%8a%80%e6%9c%af%e3%80%91Transformer%e6%9e%b6%e6%9e%84%e5%ae%97%e8%bf%b0&amp;url=http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/" target="_blank" rel="noopener noreferrer"
      title="Share on X">
      <ion-icon class="group-hover:text-primary-content" name="logo-x"></ion-icon>
    </a>
  
    <a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary"
      href="https://facebook.com/sharer/sharer.php?u=http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/" target="_blank" rel="noopener noreferrer"
      title="Share on Facebook">
      <ion-icon class="group-hover:text-primary-content" name="logo-facebook"></ion-icon>
    </a>
  
    <a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary"
      href="https://wa.me/?text=%e3%80%90LLM%e6%8a%80%e6%9c%af%e3%80%91Transformer%e6%9e%b6%e6%9e%84%e5%ae%97%e8%bf%b0%20http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/" target="_blank" rel="noopener noreferrer"
      title="Share on WhatsApp">
      <ion-icon class="group-hover:text-primary-content" name="logo-whatsapp"></ion-icon>
    </a>
  

  
  
</div>

        </div>
      </header>

      <section id="dream-single-post-content" itemprop="articleBody">
        

        <p>NLP技术从传统统计方法演进至神经网络，Word Embeddings、RNN/LSTM等模型逐步发展。Transformer凭借自注意力机制实现突破，成为里程碑，并催生了BERT、GPT等变革性模型。</p>
<h3 id="博文背景">博文背景</h3>
<p>之前在学习大模型相关技术的时候，看了很多篇对Transfomer解释的博客，但是感觉总是理解不透彻，不久之后就忘掉了，打算自己整</p>
<p>理输出一下，以便更好的理解。</p>
<p>要讲Transformer,当然要先从NLP的技术应用演进开始讲起：</p>
<h2 id="nlp技术演进背景">NLP技术演进背景</h2>
<h3 id="传统的nlp方法统计学习时期"><strong>传统的NLP方法（统计学习时期）</strong></h3>
<p>在神经网络技术普及之前，NLP领域主要依赖于<strong>基于规则的方法</strong>和<strong>统计学习方法</strong>。这些方法通过人工构建语言模型和统计方法来处理文本。</p>
<ul>
<li><strong>基于规则的系统</strong>：依赖大量的语言学规则，如语法、句法规则和词汇规则，这些规则通常由语言学家手工编写。</li>
<li><strong>统计学习方法</strong>：随着计算机能力的提升，统计方法开始兴起，常见的算法包括隐马尔可夫模型（HMM）、最大熵模型、条件随机场（CRF）等。这些方法通过学习大量标注数据中的统计规律来进行预测，比如词性标注、命名实体识别等。</li>
</ul>
<p>然而，这些方法有两个主要局限：第一，它们需要大量的手工标注数据；第二，基于规则的方法难以处理复杂的语言现象。</p>
<h3 id="神经网络的兴起"><strong>神经网络的兴起</strong></h3>
<p>神经网络的引入标志着NLP技术的根本转折。最早的神经网络应用在NLP中并不是很成功，直到2010年代深度学习的突破，才迎来了飞速的发展。</p>
<h4 id="word-embeddings">Word Embeddings</h4>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251015205211766.png" 
       alt="词嵌入技术示意" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      词嵌入技术示意
    </figcaption>
  
</figure></p>
<p>在传统方法中，词语是通过稀疏的词袋模型（Bag of Words）来表示的，这种方法无法捕捉词与词之间的关系。</p>
<p><strong>Word2Vec</strong>（2013年）和<strong>GloVe</strong>（2014年）是深度学习在NLP中的第一个重大成功。它们通过神经网络学习将词语映射到一个连续的向量空间中，称为<strong>词嵌入（Word Embedding）</strong>，使得词与词之间的关系可以通过向量空间的几何距离来反映。例如，“国王”和“王后”在词嵌入空间中的距离比“国王”和“汽车”要近，这种方式有效地捕捉了词的语义。</p>
<h4 id="rnn和lstm"><strong>RNN和LSTM</strong></h4>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251015205426121.png" 
       alt="RNN机构示意" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      RNN机构示意
    </figcaption>
  
</figure></p>
<p>**RNN（循环神经网络）**是处理序列数据（如文本）的核心架构之一，它可以通过循环连接记住前一时刻的信息。然而，RNN有一个缺陷——它难以捕捉长期依赖关系。为了解决这个问题，**LSTM（长短时记忆网络）**被提出（1997年），它能够更好地处理长期依赖问题。**GRU（门控循环单元）**是LSTM的一种变体，它简化了结构并且在某些任务中表现出与LSTM相当的效果。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251015205527512.png" 
       alt="LSMT结构示意" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      LSMT结构示意
    </figcaption>
  
</figure></p>
<h4 id="cnn">CNN</h4>
<p>**卷积神经网络（CNN）**主要用于图像处理，但它们也开始被应用于NLP任务，如文本分类和情感分析。通过将文本视作一个序列，通过卷积层提取局部特征，CNN在一些文本分析任务中非常有效。</p>
<h3 id="transformer技术">Transformer技术</h3>
<p>2017 年，Google 团队在论文**《Attention Is All You Need》<strong>中提出 Transformer，它完全摒弃递归结构，引入</strong>自注意力机制（Self-Attention）<strong>，实现了全局上下文感知与并行计算的平衡，成为 NLP 乃至深度学习领域的里程碑。<strong>Transformer</strong>是NLP技术的一次革命性突破。与RNN和LSTM不同，Transformer摒弃了序列数据的顺序处理方式，转而采用</strong>自注意力机制（Self-Attention）**，使得每个单词都能够直接与其它所有单词进行交互，从而提高了计算效率，并能捕捉更复杂的依赖关系。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/1533981-20250718224012244-220282788.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<h4 id="bert">BERT</h4>
<p><strong>BERT（Bidirectional Encoder Representations from Transformers）<strong>是基于Transformer的预训练模型，通过在大规模文本上进行双向预训练，BERT能够更好地理解文本的上下文信息。BERT的出现彻底改变了NLP模型的训练方法，许多NLP任务通过</strong>迁移学习</strong>，即先在大规模语料上预训练，再在具体任务上微调，取得了显著的效果提升。</p>
<h4 id="gpt">GPT</h4>
<p>**GPT（Generative Pre-trained Transformer)**系列是由OpenAI提出的基于Transformer架构的生成模型。与BERT的“编码器”结构不同，GPT采用了“解码器”结构，专注于生成任务，如文本生成、翻译、问答等。</p>
<p>随着模型规模的不断增大，NLP领域进入了<strong>大模型时代</strong>。例如，GPT-3有1750亿个参数，BERT的规模也达到了数百亿级别。预训练模型通过在大规模数据集上的训练，能够捕捉到丰富的语言知识，然后在特定任务上进行微调，显著提高了许多NLP任务的效果。</p>
<h1 id="transfomer是什么">Transfomer是什么</h1>
<p>直接给一个大模型给出的定义：</p>



  <blockquote>
    <p><strong>Transformer是一种用于自然语言处理（NLP）和其他序列到序列（sequence-to-sequence）任务的深度学习模型架构</strong>，它在2017年由Vaswani等人首次提出。Transformer架构引入了自注意力机制（self-attention mechanism），这是一个关键的创新，使其在处理序列数据时表现出色。
<strong>以下是Transformer的一些重要组成部分和特点：</strong></p>
<p>**自注意力机制（Self-Attention）：**这是Transformer的核心概念之一，它使模型能够同时考虑输入序列中的所有位置，而不是像循环神经网络（RNN）或卷积神经网络（CNN）一样逐步处理。自注意力机制允许模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系。
**多头注意力（Multi-Head Attention）：**Transformer中的自注意力机制被扩展为多个注意力头，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。
**堆叠层（Stacked Layers）：**Transformer通常由多个相同的编码器和解码器层堆叠而成。这些堆叠的层有助于模型学习复杂的特征表示和语义。
**位置编码（Positional Encoding）：**由于Transformer没有内置的序列位置信息，它需要额外的位置编码来表达输入序列中单词的位置顺序。
**残差连接和层归一化（Residual Connections and Layer Normalization）：**这些技术有助于减轻训练过程中的梯度消失和爆炸问题，使模型更容易训练。
**编码器和解码器：**Transformer通常包括一个编码器用于处理输入序列和一个解码器用于生成输出序列，这使其适用于序列到序列的任务，如机器翻译。</p>

  </blockquote>

<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/v2-613bffcc75eeae483f3d3de6dc1a1b54_r.jpg" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<h2 id="自顶向下理解transformer架构">自顶向下理解Transformer架构</h2>
<h3 id="从整体上理解transformer">从整体上理解Transformer</h3>
<p>假定当前我们有这样一个Transformer模型的应用场景：</p>



  <blockquote>
    <p>使用机器翻译将一句西语翻译为应用：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image.png" 
       alt="ML翻译应用示意" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      ML翻译应用示意
    </figcaption>
  
</figure></p>

  </blockquote>

<p>对该Transformer模型进行进行分解，就会找到一个编码组件、一个解码组件以及它们之间的连接。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-1.png" 
       alt="初步分解" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      初步分解
    </figcaption>
  
</figure></p>
<p>编码组件和解码组件一般都是由一堆的解码器和编码器构成的，且他们之间的数量一般是相同的。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-2.png" 
       alt="编码组件与解码组件构成" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      编码组件与解码组件构成
    </figcaption>
  
</figure></p>
<p>继续分解编码器：每个解码器的结构是完全相同的，不过他们的权重不同，每个解码器分为两个子层，一个注意力层，一个前馈神经网络层。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-3.png" 
       alt="编码器结构" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      编码器结构
    </figcaption>
  
</figure></p>
<p>编码器的输入首先流经自注意力层(self-attention) - 该层可帮助编码器在编码特定单词时查看输入句子中的其他单词。后面将详细阐述自注意力机制。</p>
<p>自注意力层的输出被馈送到前馈神经网络(Feed Forward)。完全相同的前馈网络独立应用于每个位置。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-4.png" 
       alt="编码器与解码器数据流向" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      编码器与解码器数据流向
    </figcaption>
  
</figure></p>
<p>解码器也具有这两个层，但是他们之间还有一个解码注意力层，可以帮助解码器关注输入句子的相关部分（这里我并不是特别理解）。</p>
<h3 id="将张量引入架构">将张量引入架构</h3>
<p>现在我们已经了解了模型的主要组成部分，让我们开始看看各种向量/张量以及它们如何在这些组成部分之间流动，从而将训练模型的输入转化为输出。</p>
<p>与常规的NLP模型一样，Transformer模型也会先尝试让机器读懂自然语言&ndash;即使用嵌入算法将每个书输入词转化为向量。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-5.png" 
       alt="每个单词都嵌入到一个大小为512的向量中" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      每个单词都嵌入到一个大小为512的向量中
    </figcaption>
  
</figure></p>
<p>嵌入仅发生在最底层的编码器中。所有编码器的共同抽象是它们接收一个向量列表，每个向量的大小为 512 。 在底层编码器中，这将是单词嵌入，但在其他编码器中，它将是直接位于下方的编码器的输出。此列表的大小是我们可以设置的超参数 – 基本上它将是我们训练数据集中最长句子的长度。</p>



  <blockquote class="dream-alert tip">
    <p class="heading">
      <ion-icon name="bulb-outline"></ion-icon>Tip</p>
    <p>此处也没有看很懂，后续查阅理解一下</p>
  </blockquote>

<p>将单词嵌入到输入序列之后，每个单词都会流经编码器的两层中的每一层。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-6-1760189561169-87.png" 
       alt="数据在编码器中的流向" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      数据在编码器中的流向
    </figcaption>
  
</figure></p>
<p>这里我们开始看到 Transformer 的一个关键特性，即<strong>每个位置上的单词在编码器中流经自己的路径。在自注意力层中，这些路径之间存在依赖关系。然而，前馈层没有这些依赖关系，因此各种路径可以在流经前馈层时并行执行。</strong></p>
<h3 id="自注意力机制">自注意力机制</h3>
<p>假定下面是一个机器翻译应用中要翻译的句子：</p>



  <blockquote>
    <p>“ The animal didn&rsquo;t cross the street because it was too tired”</p>

  </blockquote>

<p>这句话中的“它”指的是什么？它指的是街道还是动物？对于人类来说，这是一个简单的问题，但对于算法来说却不那么简单。</p>
<p>自注意力机制的作用就是当模型处理<code>it</code>这个词时，让模型将<code>it</code>与<code>animal</code>联系起来。</p>
<p>当模型处理每个单词（输入序列中的每个位置）时，自我注意力机制允许它查看输入序列中的其他位置以寻找有助于更好地编码该单词的线索。说人话就是<strong>对于每一个单词，模型会运算该单词与输入内容中所有单词的内容的位置关系和相关性关系</strong>。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-8-1760534908273-1.png" 
       alt="it对于输入内容中所有单词的关系" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      it对于输入内容中所有单词的关系
    </figcaption>
  
</figure></p>
<p>如上图所示，在进行单词<code>it</code>的相关性计算时，注意力机制根据计算得分，将其与<code>The animal</code>关联起来（其实不是真正意义上的关联，而是将计算结果矩阵和it的词嵌入矩阵合并，输入给下一层，对于观察者或者后面的层来说，这一步相当于把分数最高的相关性单词关联起来）。下面将详细阐述注意力机制的计算过程：</p>
<h3 id="自注意力机制的计算详解">自注意力机制的计算详解</h3>
<p>计算注意力机制的第一步是编码器给每个输入向量（在本例中理解为每个单词的嵌入）创建的三个向量（矩阵），分别为Query向量、Key向量、Value向量，这三个向量是注意力进行计算和思考的抽象概念。</p>



  <blockquote class="dream-alert caution">
    <p class="heading">
      <ion-icon name="alert-circle-outline"></ion-icon>Caution</p>
    <p>这里的新向量一般维度是小于嵌入向量的。它们的维度为64，而嵌入向量和编码器输入/输出的向量维度为512。这仅仅是一种架构选择，方便后续的多头注意力机制的计算保持恒定。</p>
  </blockquote>

<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-9-1760535614833-4.png" 
       alt="x1乘以WQ权重矩阵可得出q1，即与该词相关的“查询”向量。最终会为输入句子中的每个词创建一个“查询”、“键”和一个“值”投影" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      x1乘以WQ权重矩阵可得出q1，即与该词相关的“查询”向量。最终会为输入句子中的每个词创建一个“查询”、“键”和一个“值”投影
    </figcaption>
  
</figure></p>
<h4 id="qkv三个向量的具体意义是什么">Q|K|V三个向量的具体意义是什么</h4>
<p>这里用一个巧妙的比喻来解释：</p>



  <blockquote>
    <p>想象你在一个巨大的图书馆（图书馆就是我们要处理的一段文本或一系列信息）。</p>
<ul>
<li><strong>Query ： 你的查询请求</strong>它就是<strong>你</strong>提出的问题。比如：“我想找一本关于‘宇宙哲学’的书。”<strong>意义</strong>：<code>Query</code>代表当前需要关注什么的“意图”或“问题”。在翻译任务中，当模型在生成“美丽”这个词时，它的 <code>Query</code>就是在问：“为了生成‘美丽’这个词，我应该关注输入句子中的哪些部分？”</li>
<li><strong>Key ： 书脊上的书名和标签</strong>图书馆里每本书的书脊上都有书名和分类标签（如“哲学”、“天文”、“科幻”）。这些就是书的 <code>Key</code>。<strong>意义</strong>：<code>Key</code>是信息的“标识”或“摘要”。它用来与 <code>Query</code>进行匹配，判断这条信息是否相关。在文本中，每个单词的 <code>Key</code>向量就像是这个单词的“身份摘要”，用来回答 <code>Query</code>的提问。</li>
<li><strong>Value ： 书中的具体内容</strong>这是书里面<strong>完整的知识内容</strong>。一本标签是“宇宙哲学”的书，其 <code>Value</code>就是书中关于宇宙、生命、万物之理的详细论述。<strong>意义</strong>：<code>Value</code>是信息的“实质内容”。一旦我们通过 <code>Query</code>和 <code>Key</code>的匹配找到了相关的书，我们真正需要获取和使用的是书中的具体内容，也就是 <code>Value</code>。</li>
</ul>

  </blockquote>

<p>在上述例子中，注意力机制的工作流程如下：</p>
<ol>
<li><strong>匹配（打分）</strong>：你将你的 <code>Query</code>（“宇宙哲学”）与图书馆里所有书的 <code>Key</code>（书脊上的标签）进行相似度比较。你会给《时间简史》打高分，给《烹饪大全》打低分。</li>
<li><strong>加权（注意力权重）</strong>：根据匹配分数，你决定在每本书上花费多少“注意力”。你可能给《时间简史》100%的注意力，给《星际穿越》50%的注意力，而完全忽略《烹饪大全》。</li>
<li><strong>汇总（加权求和）</strong>：最后，你将这些注意力权重分别应用到对应的 <code>Value</code>（书的内容）上，然后汇总。你最终得到的信息，是高度集中于《时间简史》内容，并略带《星际穿越》风格的、关于“宇宙哲学”的一个<strong>全新综合理解</strong>。</li>
</ol>
<h4 id="注意力机制的矩阵计算">注意力机制的矩阵计算</h4>
<p>回归到注意力机制的计算，整个过程就很显而易见了：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251015215752561.png" 
       alt="注意力机制计算流程图" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      注意力机制计算流程图
    </figcaption>
  
</figure></p>
<p>输入序列中的<strong>每个单词</strong>（更准确地说，是它的嵌入向量）都会通过三种不同的线性变换（即乘以三个不同的权重矩阵 <em>W**Q</em>, <em>W**K</em>, <em>W**V</em>），分别生成三个对应的向量：</p>
<ul>
<li><strong>Query 向量 *q*</strong>：代表“当前位置”想获取信息的意图。</li>
<li><strong>Key 向量 *k*</strong>：代表“每个位置”所能提供信息的标识。</li>
<li><strong>Value 向量 *v*</strong>：代表“每个位置”实际拥有的信息内容。</li>
</ul>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-13-1760536378661-7.png" 
       alt="向量的产生" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      向量的产生
    </figcaption>
  
</figure></p>
<p>注意力机制计算流程：</p>
<p><strong>第1步：计算注意力分数（匹配）<strong>对于某个特定的 <code>Query</code>（比如对应“美丽”这个词的 <code>q_美丽</code>），我们用它与输入序列中</strong>所有单词</strong>的 <code>Key</code>进行点积计算相似度（<code>q_美丽 · k_我</code>, <code>q_美丽 · k_喜欢</code>, <code>q_美丽 · k_这个</code>, <code>q_美丽 · k_花园</code>）。这个分数回答了“在生成‘美丽’这个词时，与‘我’、‘喜欢’、‘这个’、‘花园’这些词的关联度有多大？”结果可能发现 <code>q_美丽 · k_花园</code>的分数最高。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251016000034810.png" 
       alt="image-20251016000034810" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      image-20251016000034810
    </figcaption>
  
</figure></p>
<p><strong>第2步：应用 Softmax（归一化为权重）<strong>将这些分数通过 Softmax 函数处理，得到一组权重系数（和为1）。这步之后，“花园”对应的权重会接近1，而其他词的权重接近0。这步将分数转化为概率分布，明确了“注意力”应该如何</strong>分配</strong>。几乎所有的注意力都集中在了“花园”上。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251016000021074.png" 
       alt="Softmax归一化处理计算出权重值" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      Softmax归一化处理计算出权重值
    </figcaption>
  
</figure></p>
<p><strong>第3步：加权求和 Value（汇总）<strong>将上一步得到的所有权重，分别与对应单词的 <code>Value</code>向量相乘，然后将所有结果相加，得到最终的</strong>注意力输出</strong>。<code>输出 = 权重_我 * v_我 + 权重_喜欢 * v_喜欢 + ... + ~1.0 * v_花园</code>这是最精妙的一步！最终的输出<strong>不是</strong>“花园”这个词的原始表示，而是所有单词 <code>Value</code>的加权平均，但其中“花园”的 <code>Value</code>占据了绝对主导。这个输出是一个<strong>融合了上下文信息的、全新的“花园”的表示</strong>。它包含了“为了理解‘美丽’，我们需要重点关注‘花园’”这一信息。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-14-1760544140467-20.png" 
       alt="矩阵形式的自注意力机制计算" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      矩阵形式的自注意力机制计算
    </figcaption>
  
</figure></p>
<h3 id="多头注意力机制">多头注意力机制</h3>
<p>还是以例句进行展示：</p>



  <blockquote>
    <p>看到句子 <strong>“那只苹果很甜，是乔布斯创立的公司”</strong> 时，你的大脑可能会并行地处理：</p>
<ol>
<li><strong>语义关联（一词多义）</strong>：“苹果”在这里指的是水果还是公司？你需要根据上下文（“甜” vs “乔布斯”）来判断。</li>
<li><strong>语法结构</strong>：“很甜”修饰的是“苹果”，构成主谓关系。</li>
<li><strong>指代关系</strong>：“是”后面的“公司”指代的是前面的“苹果”。</li>
</ol>
<p>单一的头（Single-Head）注意力机制，可以看作是<strong>一次综合性的注意力</strong>，它可能会同时捕捉到以上所有信息，但这些信息在同一个表示空间中是混杂在一起的。</p>

  </blockquote>

<p><strong>多头注意力（Multi-Head Attention）的核心思想就是：</strong></p>



  <blockquote>
    <p><strong>“与其让一个注意力机制一次性完成所有信息的捕捉，不如将模型划分为多个‘头’（子空间），让每个头专注于学习一种特定的注意力模式。最后再将所有头的结果合并起来，从而得到更丰富、更细腻的上下文表示。”</strong></p>

  </blockquote>

<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-20251016000626298.png" 
       alt="image-20251016000626298" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      image-20251016000626298
    </figcaption>
  
</figure></p>
<h4 id="多头注意力机制的计算">多头注意力机制的计算</h4>
<p><strong>第1步：初始化</strong></p>
<p>假设输入序列有 <code>n</code>个词，每个词被表示为一个维度是 <code>d_model</code>（例如512）的向量。那么输入矩阵 <code>X</code>的维度是 <code>[n, d_model]</code>。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-15-1760545117765-23.png" 
       alt="借助多头注意力机制，为每个头维护单独的 Q/K/V 权重矩阵，从而产生不同的 Q/K/V 矩阵。与之前一样，将 X 乘以 WQ/WK/WV 矩阵以生成 Q/K/V 矩阵。" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      借助多头注意力机制，为每个头维护单独的 Q/K/V 权重矩阵，从而产生不同的 Q/K/V 矩阵。与之前一样，将 X 乘以 WQ/WK/WV 矩阵以生成 Q/K/V 矩阵。
    </figcaption>
  
</figure></p>
<p><strong>第2步：创建多个头（并行操作）</strong></p>
<ol>
<li><strong>线性投影：</strong> 对于每个头 <code>i</code>（假设共有 <code>h</code>个头，例如8个），我们使用三组<strong>独立的</strong>权重矩阵 <code>W_i^Q</code>, <code>W_i^K</code>, <code>W_i^V</code>将原始的 <code>Q, K, V</code>分别投影到<strong>更低的维度</strong>。通常，每个头的维度会变为 <code>d_k = d_v = d_model / h</code>（例如512/8=64）。这样做的好处是总计算量与单头注意力相似。</li>
<li><strong>独立计算注意力：</strong> 在每个投影后的低维空间里，分别执行标准的缩放点积注意力（Scaled Dot-Product Attention）操作：<strong>计算注意力分数</strong>：<code>Attention(Q * W_i^Q, K * W_i^K, V * W_i^V)</code>这样，每个头都会产生一个输出矩阵 <code>head_i</code>，其维度是 <code>[n, d_v]</code>（例如 <code>[n, 64]</code>）。</li>
</ol>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-16-1760545154207-26.png" 
       alt="独立计算" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      独立计算
    </figcaption>
  
</figure></p>
<p><strong>第3步：合并输出</strong></p>
<ol>
<li><strong>拼接：</strong> 将所有 <code>h</code>个头的输出矩阵 <code>head_1, head_2, ..., head_h</code>在<strong>特征维度</strong>上拼接起来，得到一个维度为 <code>[n, h * d_v] = [n, d_model]</code>的大矩阵。</li>
<li><strong>最终线性投影：</strong> 将这个拼接后的大矩阵通过一个<strong>可学习的权重矩阵 <code>W^O</code></strong> 进行线性变换，得到最终的输出。<code>W^O</code>的维度通常是 <code>[d_model, d_model]</code>。这个步骤的作用是融合所有头的信息，并允许模型学习如何最有效地组合它们。</li>
</ol>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-17-1760545184996-29.png" 
       alt="矩阵拼接与线性投影" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      矩阵拼接与线性投影
    </figcaption>
  
</figure></p>
<h4 id="多头注意力的优势">多头注意力的优势</h4>
<ol>
<li><strong>增强建模能力</strong>：允许模型在不同的表示子空间里协同关注来自不同位置的信息，大大扩展了模型的表示能力。</li>
<li><strong>并行计算</strong>：多个头之间互不依赖，可以完全并行计算，效率极高。</li>
<li><strong>可解释性</strong>：通过可视化不同头的注意力权重，我们可以直观地看到模型学习了哪些类型的语法或语义关系（如指代关系、形容词修饰关系等），这为理解模型提供了一扇窗口。</li>
</ol>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-18-1760545226703-32.png" 
       alt="多头注意力机制的完整展示" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      多头注意力机制的完整展示
    </figcaption>
  
</figure></p>
<h4 id="常见的误区">常见的误区</h4>
<p><strong>多头注意力的意思就是输入内容的每一个单词都会作为一个头去计算和别的单词的相关性吗？</strong></p>



  <blockquote>
    <p>这是一个非常常见的误解，这个描述更接近“自注意力机制”本身，而不是“多头”。</p>
<p>在自注意力中，<strong>每个词</strong>都会扮演两个角色：<strong>主动查询者</strong>：它用自己的 Query 向量去“询问”序列中的所有词（包括自己）：“你们谁跟我最相关？”<strong>被查询对象</strong>：它的 Key 和 Value 向量被其他词的 Query 向量用来计算相关性。所以，自注意力机制<strong>本身</strong>就保证了每个词都会与所有其他词进行交互。</p>

  </blockquote>

<p><strong>正确的理解：多头（Multi-Head）</strong></p>
<p>“多头”是指在<strong>已经有的自注意力机制之上</strong>，再增加一个“并行化”的维度。它的意思是：</p>



  <blockquote>
    <p><strong>从多个不同的“视角”或“侧面”来并行地计算这种相关性，而不仅仅只计算一次。</strong></p>

  </blockquote>

<p>还是举一个栗子：</p>



  <blockquote>
    <p>假设我们要分析一个“苹果”。</p>
<ul>
<li><strong>单头注意力（Single-Head）</strong>：相当于你<strong>一个人</strong>，综合地从颜色、形状、大小、气味等多个方面来观察这个苹果，然后得出一个整体的结论。</li>
<li><strong>多头注意力（Multi-Head，比如8个头）</strong>：相当于你组建了一个<strong>8人的专家团队</strong>，让他们<strong>同时</strong>观察这个苹果：<strong>头1（颜色专家）</strong>：只专注于分析苹果的颜色（红、黄、绿），给出颜色方面的报告。<strong>头2（形状专家）</strong>：只专注于分析苹果的形状（圆、歪），给出形状方面的报告。<strong>头3（质地专家）</strong>：只专注于触摸苹果的表面（光滑、粗糙）。<strong>头4（语义专家，针对文本）</strong>：专注于分析“苹果”这个词是指水果还是科技公司。<strong>头5（语法专家）</strong>：专注于分析“苹果”在句子中是主语还是宾语。&hellip;（还有其他专家）最后，你将这8份<strong>不同视角</strong>的专家报告汇总起来，得到对这个苹果最全面、最深入的理解。</li>
</ul>

  </blockquote>

<p>再举一个更具体的例子，以上面的<code>我喜欢这个美丽花园</code>作为输入，在Transformer模型中:</p>
<ol>
<li>
<p>**第一步：序列 <code>[我， 喜欢， 美丽， 花园]</code>输入模型。这是基础输入。</p>
</li>
<li>
<p><strong>第二步</strong>：模型不会只计算<strong>一套</strong>注意力权重。它会同时初始化8个（例如）独立的“专家团队”（即8个头）。每个“头”都有自己的一套独立的参数（独立的 <code>W_Q</code>, <code>W_K</code>, <code>W_V</code>矩阵），这保证了它们会学习关注不同的方面。<strong>每个头都会独立地为序列中的每个词计算一次它与其他所有词的相关性</strong>。也就是说，<strong>每个头都会生成一个完整的、但视角不同的注意力权重矩阵</strong>。</p>
<p>例如，在处理“美丽”这个词时：<strong>头1</strong> 可能计算出 <code>花园</code>与 <code>美丽</code>的相关性最强（关注<strong>语义搭配</strong>）。<strong>头2</strong> 可能计算出 <code>喜欢</code>与 <code>美丽</code>的相关性最强（关注<strong>语法动宾关系</strong>）。<strong>头3</strong> 可能更关注 <code>这个</code>与 <code>美丽</code>的关系（关注<strong>指代或修饰关系</strong>）。</p>
</li>
<li>
<p><strong>第三步</strong>：将8个头得到的8组结果拼接起来，再通过一个线性层融合，得到最终的输出。这个输出融合了8种不同的关系信息。</p>
</li>
</ol>
<table>
  <thead>
      <tr>
          <th style="text-align: left">机制</th>
          <th style="text-align: left">核心问题</th>
          <th style="text-align: left">比喻</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>自注意力</strong></td>
          <td style="text-align: left"><strong>一个词如何与序列中所有词交互？</strong></td>
          <td style="text-align: left">保证团队里的每个人（每个词）都能和所有其他人交流。</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>多头注意力</strong></td>
          <td style="text-align: left"><strong>我们能否从多个不同角度来评估这种交互？</strong></td>
          <td style="text-align: left">组建一个专家团队，每个人从不同视角（颜色、形状等）同时评估这些交流。</td>
      </tr>
  </tbody>
</table>
<h3 id="位置编码">位置编码</h3>
<p>到目前为止，我们所描述的模型只缺少一件事，那就是解释输入序列中单词顺序的方法。</p>
<p>为了解决这个问题，Transformer 模型对每个输入的向量都添加了一个向量。这些向量遵循模型学习到的特定模式，有助于确定每个单词的位置，或者句子中不同单词之间的距离。这种做法背后的直觉是：将这些表示位置的向量添加到词向量中，得到了新的向量，这些新向量映射到 Q/K/V，然后计算点积得到 attention 时，可以提供有意义的信息。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-21-1760545770287-35.png" 
       alt="为了让模型了解单词的顺序，添加了位置编码向量——其值遵循特定的模式。" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      为了让模型了解单词的顺序，添加了位置编码向量——其值遵循特定的模式。
    </figcaption>
  
</figure></p>
<p>假设嵌入的维数为 4，则实际的位置编码将如下所示：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-22-1760545817067-38.png" 
       alt="当嵌入向量的长度是4的时候，位置编码的长度也是4" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      当嵌入向量的长度是4的时候，位置编码的长度也是4
    </figcaption>
  
</figure></p>
<p>更直观的展示一下位置编码：</p>
<p>在下图中，每一行对应一个向量的位置编码。因此，第一行将是我们要添加到输入序列中第一个单词的嵌入的向量。每行包含 512 个值 - 每个值介于 1 和 -1 之间。对它们进行了颜色编码，以便模式清晰可见。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-23-1760545930841-41.png" 
       alt="这是 20 个字（行）的位置编码的真实示例，嵌入大小为 512（列）。您可以看到它从中间一分为二。这是因为左半部分的值由一个函数（使用正弦）生成，右半部分的值由另一个函数（使用余弦）生成。然后将它们连接起来以形成每个位置编码向量。" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      这是 20 个字（行）的位置编码的真实示例，嵌入大小为 512（列）。您可以看到它从中间一分为二。这是因为左半部分的值由一个函数（使用正弦）生成，右半部分的值由另一个函数（使用余弦）生成。然后将它们连接起来以形成每个位置编码向量。
    </figcaption>
  
</figure></p>
<h3 id="残差">残差</h3>
<p>回到Transformer的架构图，可以发现：编码器结构中编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization）。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-25-1760546105022-44.png" 
       alt="编码器结构" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      编码器结构
    </figcaption>
  
</figure></p>
<p>将自注意力机制的相关向量和层进行可视化：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-26-1760546366786-47.png" 
       alt="编码器向量可视化" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      编码器向量可视化
    </figcaption>
  
</figure></p>
<p>是不是看的云里雾里，其实很简单，我仍然举一个通俗的例子：</p>



  <blockquote>
    <p>假设你（模型）要学习做一道菜，比如“西红柿炒鸡蛋”。我们把做菜的过程看成网络的一层。</p>
<p><strong>1. 没有残差连接时（传统方式）：</strong></p>
<ul>
<li>你每次做菜，都是<strong>从零开始</strong>。</li>
<li>师傅尝了你的菜说：“味道不够，需要改进。”</li>
<li>但你不知道具体哪一步出了问题。是盐放少了？火候不对？鸡蛋炒老了？你需要<strong>猜测并调整整个做菜流程</strong>。这非常困难，而且很容易改得面目全非，甚至还不如上一版。</li>
</ul>
<p><strong>2. 有残差连接时（Transformer的方式）：</strong></p>
<ul>
<li>你做了一道“西红柿炒鸡蛋”（我们叫它 <strong>版本A</strong>）。</li>
<li>师傅尝了后说：“味道不够，需要改进。”</li>
<li>但现在，你不是从头开始做。你的任务是：<strong>在版本A的基础上，只做一个小小的“改进”或“补丁”</strong>。</li>
<li>**具体步骤是：**<strong>保留原版</strong>：你把做好的“版本A”完整地留在盘子里。<code>（这就是 x，输入）</code><strong>制作“改进包”</strong>：你另外拿个小碗，调一点点更鲜的酱油汁，或者切一点葱花。<code>（这就是 F(x)，要学习的“残差”或“变化”）</code><strong>合并</strong>：最后，你把小碗里的“改进包”浇到“版本A”上。<code>（这就是 x + F(x)，输出 = 输入 + 变化）</code><strong>最终菜品 = 原来的菜 (x) + 改进的调料 (F(x))</strong></li>
</ul>

  </blockquote>

<p>在上面的示意图中，在Transformer的每一层（比如注意力层），发生的事情就是：</p>
<p><strong>输入（一句话的词向量）</strong> &ndash;进入&ndash;&gt; <strong>【本层的复杂计算】</strong> &ndash;产生&ndash;&gt; <strong>一个“变化量”</strong></p>
<p>然后：<strong>本层的最终输出 = 输入 + 变化量</strong></p>
<p>这个“+”号，就是<strong>残差连接</strong>。它确保了这一层无论怎么折腾，最初输入的信息都原封不动地保留了一份，并传递到了下一层。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-27-1760546832738-50.png" 
       alt=" 2 个堆叠的编码器和解码器组成的 Transformer的数据流" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
       2 个堆叠的编码器和解码器组成的 Transformer的数据流
    </figcaption>
  
</figure></p>
<h4 id="为什么残差如此重要">为什么残差如此重要？</h4>
<p>残差连接解决了训练<strong>深度神经网络</strong>时的两个核心难题：</p>



  <blockquote>
    <p><strong>1. 梯度消失/爆炸问题</strong></p>
<ul>
<li><strong>没有残差连接</strong>：在反向传播时，梯度需要从最深层（第N层）一层一层地传回最浅层（第1层）。这个过程就像穿过一个漫长的、信号会不断衰减的隧道。当网络很深时，梯度可能会变得极小（消失）或极大（爆炸），导致浅层参数无法得到有效的更新。</li>
<li><strong>有残差连接</strong>：梯度可以通过残差路径这条“捷径”直接、无损地传回更早的层。这确保了即使网络有几十甚至上百层，所有层都能获得有效的梯度信号，使得训练非常深的模型成为可能。</li>
</ul>
<p><strong>2. 网络退化问题</strong></p>
<ul>
<li>即使解决了梯度消失，实验发现，单纯增加网络深度反而会导致训练和测试误差都变大。这不是过拟合，而是网络<strong>退化</strong>了——更深的网络表现反而不如较浅的网络。</li>
<li><strong>残差连接的妙处</strong>：如果一个更深的网络的最优解就是简单地模仿一个较浅的网络（即让新增的层什么也不做），那么学习 <code>F(x) = 0</code>要比学习 <code>H(x) = x</code>容易得多！因为 <code>F(x) = 0</code>意味着让权重逼近零即可。这样，更深的网络<strong>至少不会比浅层网络差</strong>。如果更深层有用，模型再去学习一个非零的 <code>F(x)</code>。</li>
</ul>
<p>它是 Transformer 能够构建得非常深（例如 GPT-3 有 96 层）、从而拥有强大能力的关键技术支柱之一。</p>

  </blockquote>

<h3 id="解码器">解码器</h3>
<p>现在我们已经介绍了编码器方面的大部分概念，也基本上知道了解码器组件的工作原理。让我们看看它们是如何协同工作的。</p>
<p>编码器首先处理输入序列。然后，顶部编码器的输出被转换为一组注意向量 K 和 V。每个解码器将在其“编码器-解码器注意”层中使用它们，这有助于解码器将注意力集中在输入序列中的适当位置：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/2-1760547169977-53.gif" 
       alt="编码器的协同工作" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      编码器的协同工作
    </figcaption>
  
</figure></p>
<p>还是举个例子方便理解</p>



  <blockquote>
    <h4 id="第1步编码器处理输入序列">第1步：编码器处理输入序列</h4>
<p>法语情报文档 <code>&quot;J'aime le beau jardin&quot;</code>（“我喜欢美丽的花园”）被送入编码器团队。这个团队由多个（比如6个）分析专家（编码器层）堆叠而成。</p>
<ul>
<li><strong>过程</strong>：第一个分析专家（编码器层1）阅读原始文档，进行初步分析，输出一份初步的“分析笔记”。这份笔记传递给第二个专家（编码器层2），他在前一个专家的分析基础上进行更深入、更综合的分析。以此类推，直到最顶层的专家（编码器层6）完成分析。</li>
<li><strong>结果</strong>：顶层编码器输出的，不再是简单的单词表示，而是<strong>一份富含上下文、已经充分理解了整句话含义的“终极分析报告”</strong>。这份报告里的每个词（如 <code>&quot;J'aime&quot;</code>, <code>&quot;beau&quot;</code>, <code>&quot;jardin&quot;</code>）都已经被赋予了基于整个句子上下文的意义。</li>
</ul>
<h4 id="第2步生成注意向量-k-和-v">第2步：生成注意向量 K 和 V</h4>
<p>这份“终极分析报告”被转换成两份特殊的情报摘要：<strong>K（密钥）</strong> 和 <strong>V（值）</strong>。你可以理解为：<strong>K</strong> 像是报告内容的<strong>精确索引</strong>或<strong>关键词列表</strong>。它帮助快速定位信息。<strong>V</strong> 像是报告内容的<strong>实质信息</strong>或<strong>详细内容</strong>本身。</p>
<ul>
<li><strong>目的</strong>：这样做的目的是为了让解码器（报告撰写员）能够高效地查询这些信息，而无需重新阅读整个复杂的分析报告。</li>
</ul>
<h4 id="第3步解码器逐步生成并查询编码器">第3步：解码器逐步生成，并查询编码器</h4>
<p>“每个解码器将在其‘编码器-解码器注意’层中使用它们，这有助于解码器将注意力集中在输入序列中的适当位置。”现在，报告撰写员（解码器）开始用英语一个字一个字地写报告。<strong>这是最关键的一步！</strong></p>
<ul>
<li><strong>具体过程（假设已经生成了前两个词 “I love”）：</strong> <strong>解码器自注意力</strong>：撰写员先看看自己已经写了什么（<code>&quot;I love&quot;</code>），确保下文连贯。（这是解码器的自注意力层）<strong>关键一步：询问编码器</strong>：接下来，撰写员要写下一个词了。他思考：“基于我目前写的 <code>‘I love’</code>，我应该去法语文档的哪个部分寻找最重要的信息来确定下一个词？”他拿着当前的状态（对应一个 Query，Q），去查询编码器提供的那份索引K。通过比对，他发现他的Q与编码器K中代表 <code>&quot;beau&quot;</code>（美丽）和 <code>&quot;jardin&quot;</code>（花园）的键最匹配。特别是 <code>&quot;jardin&quot;</code>（花园）匹配度最高。于是，他将<strong>注意力集中</strong>到编码器信息V中与 <code>&quot;jardin&quot;</code>相关的实质内容上。<strong>生成词语</strong>：综合了自身已写的内容（<code>&quot;I love&quot;</code>）和从编码器那里聚焦的信息（<code>&quot;jardin&quot;</code>的语义），他判断下一个词应该是 <code>&quot;the beautiful garden&quot;</code>，并先写出下一个词 <code>&quot;the&quot;</code>。</li>
</ul>

  </blockquote>

<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/transformer_decoding_2-1760547718150-56.gif" 
       alt="alt text" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      alt text
    </figcaption>
  
</figure></p>



  <blockquote class="dream-alert note">
    <p class="heading">
      <ion-icon name="information-circle-outline"></ion-icon>Note</p>
    <p>解码器中的自注意力层的操作方式与编码器中的自注意力层略有不同：</p>
<p>在解码器中，自注意力层仅允许关注输出序列中的较早位置。这是通过<code>-inf</code>在自注意力计算中的 <code>softmax </code>步骤之前屏蔽未来位置（将其设置为0）来实现的。“编码器-解码器注意”层的工作原理与多头自注意类似，不同之处在于它从其下方的层创建查询矩阵，并从编码器堆栈的输出中获取键和值矩阵。</p>
  </blockquote>

<h3 id="最后的线性层和-softmax-层">最后的线性层和 Softmax 层</h3>
<p>解码器堆栈输出一个浮点向量。我们如何将其转换成一个单词？这是最后一个线性层的工作，后面是 Softmax 层。</p>
<p>线性层是一个简单的完全连接的神经网络，它将解码器堆栈产生的向量投影到一个更大的向量中，称为 logits 向量。</p>
<p>假设我们的模型知道从训练数据集中学习到的 10,000 个独特的英语单词（我们模型的“输出词汇表”）。这将使 logits 向量宽度达到 10,000 个单元格 - 每个单元格对应一个独特单词的分数。这就是我们解释线性层之后的模型输出的方式。</p>
<p>然后，softmax 层将这些分数转换为概率（所有分数均为正数，总和为 1.0）。选择概率最高的单元格，并生成与其关联的单词作为此时间步骤的输出。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/llm%E6%8A%80%E6%9C%AFtransformer%E6%9E%B6%E6%9E%84%E5%AE%97%E8%BF%B0/image-30-1760548019903-59.png" 
       alt="生成的向量作为解码器堆栈的输出。然后将其转换为输出字。" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      生成的向量作为解码器堆栈的输出。然后将其转换为输出字。
    </figcaption>
  
</figure></p>
<h2 id="总结">总结</h2>
<p><strong>总之，Transformer 是一个基于自注意力的编码器-解码器架构。编码器负责将输入序列压缩为上下文向量，解码器则通过交叉注意力机制，基于编码器的输出和已生成的内容，自回归地生成目标序列。其高效、强大的特性使其成为自然语言处理领域的绝对主流。</strong></p>
<h2 id="博客学习网站视频-推荐">博客|学习网站|视频 推荐</h2>
<p><strong>博客：</strong></p>
<ul>
<li><a href="https://blog.eimoon.com/p/transformer-explained/" target="_blank">图解Transformer | 图形化深入浅出地解释 Transformer 模型的工作原理</a>
</li>
<li><a href="https://blog.csdn.net/Kuo_Jun_Lin/article/details/114241287?spm=1001.2014.3001.5506" target="_blank">序列模型之王 - Transfomer 全细节详解-CSDN博客</a>
</li>
<li><a href="https://blog.csdn.net/weixin_42475060/article/details/121101749" target="_blank">【超详细】【原理篇&amp;实战篇】一文读懂Transformer-CSDN博客</a>
</li>
</ul>
<p>视频：</p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1NCgVzoEG9/?share_source=copy_web&amp;vd_source=a06df7b174b0e55e45242729b8ce1758" target="_blank">一小时从函数到Transformer！一路大白话彻底理解AI原理</a>
</li>
</ul>


        
      </section>

      

      
    </article>
  </div>

  <div
    x-data="tocHighlighter()"
    @scroll.window="debouncedScroll"
    class="hidden lg:flex lg:flex-col lg:items-end lg:self-start"
  >
    
  </div>
</div>


            
<footer class="flex justify-between items-center gap-2 px-4 py-12">

  <div>
  
  <p>
    © 2021 - 2025 古月月仔的博客
  </p>
  

  
  <p class="text-sm">
    🌱
    <span class="text-base-content/60">
      Powered by <a class="hover:underline" href="https://gohugo.io/" target="_blank">Hugo</a> with theme
      <a class="hover:underline" href="https://github.com/g1eny0ung/hugo-theme-dream" target="_blank">Dream</a>.</span
    >
  </p>
  
</div>

  <div
  x-data="{ icons: [
    { name: 'sunny', status: 'n' },
    { name: 'moon', status: 'y' },
    { name: 'desktop', status: 'auto' }
  ] }"
  class="flex items-center gap-2 h-[32px] px-2 bg-base-100 border border-base-content/30 rounded-full"
>
  <template x-for="icon in icons">
    <div
      role="button"
      tabindex="0"
      :aria-label="'Select ' + icon.name + ' mode'"
      class="group inline-flex justify-center items-center p-1 rounded-full cursor-pointer hover:bg-primary"
      :class="$store.darkMode.icon() === icon.name && 'bg-primary'"
      @click="$store.darkMode.toggle(icon.status)"
    >
      <ion-icon
        :name="`${icon.name}-outline`"
        class="group-hover:text-primary-content"
        :class="$store.darkMode.icon() === icon.name && 'text-primary-content'"
      >
      </ion-icon>
    </div>
  </template>
</div>

</footer>

          </div>
        </div>
        <div class="back">
          <div class="container">
            
            <div class="dream-grid dream-grid-about">
  
  
  
  <div class="w-full md:w-1/2 lg:w-1/3 xl:w-1/4 p-4 dream-column">
    <article
      class="card bg-base-100 hover:bg-base-content/10 shadow-xl dark:border dark:border-base-content/30"
    >
      <div class="card-body">
        <div class="card-title">关于我</div>

        <div class="prose dark:prose-invert">
          <ul>
<li>我是<strong>古月月仔</strong></li>
<li><strong>Ethan Hu</strong></li>
<li>分享技术学习笔记与生活点滴</li>
<li><strong>现居</strong>： 上海 中国</li>
<li><strong>家乡</strong>： 平遥 山西</li>
</ul>

        </div>
      </div>
    </article>
  </div>
  
  <div class="w-full md:w-1/2 lg:w-1/3 xl:w-1/4 p-4 dream-column">
    <article
      class="card bg-base-100 hover:bg-base-content/10 shadow-xl dark:border dark:border-base-content/30"
    >
      <div class="card-body">
        <div class="card-title">在用的学习工具</div>

        <div class="prose dark:prose-invert">
          <ul>
<li>📝 <a href="https://typora.io/" target="_blank">Typora</a>
 —— 极致简洁的 Markdown 编辑器，助力沉浸式文档撰写与知识记录。</li>
<li>📓 <a href="https://www.notion.so/" target="_blank">Notion</a>
 —— 一站式工作空间，用于搭建个人知识库、项目管理与深度协作。</li>
<li>🔗 <a href="https://n8n.io/" target="_blank">N8N</a>
 —— 强大的基于节点的自动化工作流工具，轻松实现不同应用间的逻辑联动。</li>
<li>🤖 <a href="https://gemini.google.com/" target="_blank">Gemini</a>
 —— 智能 AI 助手，在代码辅助、创意激发与信息检索中提供强力支撑。</li>
</ul>

        </div>
      </div>
    </article>
  </div>
  
  <div class="w-full md:w-1/2 lg:w-1/3 xl:w-1/4 p-4 dream-column">
    <article
      class="card bg-base-100 hover:bg-base-content/10 shadow-xl dark:border dark:border-base-content/30"
    >
      <div class="card-body">
        <div class="card-title">我的爱好</div>

        <div class="prose dark:prose-invert">
          <ul>
<li>🚀 喜欢折腾各种好玩的技术</li>
<li>📸 业余摄影爱好者</li>
<li>🎮 各类游戏玩家</li>
<li>💻 数码产品折腾爱好者</li>
<li>📚 阅读：赫尔曼·黑塞 &amp; 阿尔贝·加缪</li>
<li>🎞️ 追番中：《电锯人：蕾塞篇》</li>
<li>🎬 经典重温：《命运石之门》</li>
</ul>

        </div>
      </div>
    </article>
  </div>
  
  <div class="w-full md:w-1/2 lg:w-1/3 xl:w-1/4 p-4 dream-column">
    <article
      class="card bg-base-100 hover:bg-base-content/10 shadow-xl dark:border dark:border-base-content/30"
    >
      <div class="card-body">
        <div class="card-title">最近正在学</div>

        <div class="prose dark:prose-invert">
          <ul>
<li>🎨 <a href="https://games104.boomingtech.com/" target="_blank">GAMES104</a>
 —— 现代游戏引擎入门。</li>
<li>🏔️ <a href="https://www.sidefx.com/learn/" target="_blank">Houdini 基础</a>
 —— 学习程序化建模与特效。</li>
<li>🤗 <a href="https://huggingface.co/learn" target="_blank">Hugging Face</a>
 —— 探索开源 AI 社区。</li>
</ul>

        </div>
      </div>
    </article>
  </div>
  
  

  

  
</div>

            

            
<footer class="flex justify-between items-center gap-2 px-4 py-12">

  <div>
  
  <p>
    © 2021 - 2025 古月月仔的博客
  </p>
  

  
  <p class="text-sm">
    🌱
    <span class="text-base-content/60">
      Powered by <a class="hover:underline" href="https://gohugo.io/" target="_blank">Hugo</a> with theme
      <a class="hover:underline" href="https://github.com/g1eny0ung/hugo-theme-dream" target="_blank">Dream</a>.</span
    >
  </p>
  
</div>

  <div
  x-data="{ icons: [
    { name: 'sunny', status: 'n' },
    { name: 'moon', status: 'y' },
    { name: 'desktop', status: 'auto' }
  ] }"
  class="flex items-center gap-2 h-[32px] px-2 bg-base-100 border border-base-content/30 rounded-full"
>
  <template x-for="icon in icons">
    <div
      role="button"
      tabindex="0"
      :aria-label="'Select ' + icon.name + ' mode'"
      class="group inline-flex justify-center items-center p-1 rounded-full cursor-pointer hover:bg-primary"
      :class="$store.darkMode.icon() === icon.name && 'bg-primary'"
      @click="$store.darkMode.toggle(icon.status)"
    >
      <ion-icon
        :name="`${icon.name}-outline`"
        class="group-hover:text-primary-content"
        :class="$store.darkMode.icon() === icon.name && 'text-primary-content'"
      >
      </ion-icon>
    </div>
  </template>
</div>

</footer>

          </div>
        </div>
      </div>
    </div>

    <script>
  window.lightTheme = "emerald"
  window.darkTheme = "forest"
</script>


  <script src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js"></script>

  
  
  <script src="/js/grid.js"></script>




<script src="/js/main.js"></script>
    







<script src="/js/toc.js"></script>




  




    

    
      <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [
        ['\\[', '\\]'],
        ['$$', '$$'],
      ], 
      inlineMath: [
        ['\\(', '\\)'],
        ['$', '$'],
      ], 
    },
  }
</script>

    

    

    

    <script type="module" src="https://cdn.jsdelivr.net/npm/ionicons@7.4.0/dist/ionicons/ionicons.esm.js" integrity="sha256-/IFmi82bIhdYWctu0UddSlJqpnzWm7Vh2C4CM32wF/k=" crossorigin="anonymous"></script>
    <script nomodule src="https://cdn.jsdelivr.net/npm/ionicons@7.4.0/dist/ionicons/ionicons.js" integrity="sha256-mr7eJMX3VC3F7G32mk4oWp1C6a2tlMYxUdptfT7uKI8=" crossorigin="anonymous"></script>
  </body>
</html>
