<!DOCTYPE html>
<html lang="zh-cn"
  x-data
  :class="$store.darkMode.class()"
  :data-theme="$store.darkMode.theme()">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【深度学习】常见的激活函数 | 古月月仔的博客</title>

    

<link rel="canonical" href="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" />


<meta name="author" content="古月月仔" />
<meta name="description" content="激活函数为神经网络引入非线性，使其能够学习复杂模式。它分为饱和与非饱和两类，前者如Sigmoid和tanh，后者如ReLU。使用激活函数可避免网络退化为线性模型，并解决梯度消失等问题，从而增强模型的表达能力。" />
<meta name="keywords" content="机器学习,激活函数,Sigmoid,Softmax">


<meta name="generator" content="Hugo 0.153.1">

<meta property="og:url" content="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">
  <meta property="og:site_name" content="古月月仔的博客">
  <meta property="og:title" content="【深度学习】常见的激活函数">
  <meta property="og:description" content="激活函数为神经网络引入非线性，使其能够学习复杂模式。它分为饱和与非饱和两类，前者如Sigmoid和tanh，后者如ReLU。使用激活函数可避免网络退化为线性模型，并解决梯度消失等问题，从而增强模型的表达能力。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-01-30T00:00:00+08:00">
    <meta property="article:modified_time" content="2025-01-30T00:00:00+08:00">
    <meta property="article:tag" content="机器学习">
    <meta property="article:tag" content="激活函数">
    <meta property="article:tag" content="Sigmoid">
    <meta property="article:tag" content="Softmax">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="【深度学习】常见的激活函数">
  <meta name="twitter:description" content="激活函数为神经网络引入非线性，使其能够学习复杂模式。它分为饱和与非饱和两类，前者如Sigmoid和tanh，后者如ReLU。使用激活函数可避免网络退化为线性模型，并解决梯度消失等问题，从而增强模型的表达能力。">




<link rel="stylesheet" href="/css/output.css" />



<script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3/dist/cdn.min.js"></script>
<link rel="stylesheet" href="/css/custom.css">
<script src="/js/code_block.js" defer></script>

    


<style>
  pre {
    padding: 1em;
    overflow: auto;
  }
</style>









    

    <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3/dist/cdn.min.js" integrity="sha256-4EHxtjnR5rL8JzbY12OKQJr81ESm7JBEb49ORPo29AY=" crossorigin="anonymous"></script>
  </head>
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js"></script>

<style>
   
  .medium-zoom-overlay {
    z-index: 100;
    background: rgba(0, 0, 0, 0.8) !important;
  }
  .medium-zoom-image--opened {
    z-index: 101;
  }
</style>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    
    const zoom = mediumZoom('.zoomable', {
      margin: 24,          
      background: '#000',  
      scrollOffset: 40,    
    });
  });
</script>
  <body x-data="{
    flip: false,
  }">
    
    <div id="dream-global-bg"></div>

    
<nav class="mt-4 lg:mt-8 py-4">

  
  <div class="container flex justify-between px-4">
  
    <section class="flex items-center gap-4">
      <div class="avatar cursor-pointer hover:avatar-online" @click="flip = !flip" title="Flip it!">
        <div class="h-10 rounded-full">
          <img src="/img/ChipDog.jpg" alt="古月月仔的博客" />
        </div>
      </div>

      
      <div>
        
        <a href="http://localhost:1313/" class="text-lg font-semibold cursor-pointer">
          古月月仔的博客
        </a>
        
        
        <div class="text-base-content/60 text-sm">计算机在校生 || 游戏程序员
记录从基础到进阶的每一份思考。</div>
        
      </div>
      
    </section>

    
    

    <div class="dropdown dropdown-end sm:hidden">
      <div tabindex="0" role="button" class="btn btn-ghost btn-square" aria-label="Select an option">
        <ion-icon name="menu" class="text-2xl"></ion-icon>
      </div>
      <ul tabindex="0" class="dropdown-content menu w-36 bg-base-100 rounded-box z-1 shadow-md">
        







<li>
  <div role="link" tabindex="0" class="inline-flex items-center p-2 cursor-pointer" @click="flip = !flip" title="About">
    <ion-icon name="information-circle"></ion-icon>About</div>
</li>





















<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/posts" title="Archives">
    <ion-icon name="archive"></ion-icon>
    Archives
  </a>
</li>




<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/categories" title="All Categories">
    <ion-icon name="grid"></ion-icon>
    All Categories
  </a>
</li>




<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/tags" title="All Tags">
    <ion-icon name="pricetags"></ion-icon>
    All Tags
  </a>
</li>






      </ul>
    </div>
    <section class="hidden sm:flex sm:items-center sm:gap-2 md:gap-4">
      

      
      




<div role="link" tabindex="0" class="text-sm font-semibold cursor-pointer hover:underline" @click="flip = !flip" title="About">About</div>





      
      





      
      





      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/posts" title="Archives">
  <ion-icon class="group-hover:text-primary-content" name="archive"></ion-icon>
</a>


      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/categories" title="All Categories">
  <ion-icon class="group-hover:text-primary-content" name="grid"></ion-icon>
</a>


      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/tags" title="All Tags">
  <ion-icon class="group-hover:text-primary-content" name="pricetags"></ion-icon>
</a>


      

      

      
    </section>
  </div>
</nav>


    <div class="flip-container" :class="{ 'flip-it': flip }">
      <div class="flipper">
        <div class="front">
          <div class="container">
            
<div class="lg:grid lg:grid-cols-4 gap-4 mt-4 px-4">
  <div class="hidden lg:block">
    
  </div>

  <div class="lg:col-span-2">
    <article class="mx-auto prose prose-quoteless dark:prose-invert" id="dream-single-post-main" itemscope itemtype="http://schema.org/Article">
      
  <meta itemprop="name" content="【深度学习】常见的激活函数">
  <meta itemprop="description" content="激活函数为神经网络引入非线性，使其能够学习复杂模式。它分为饱和与非饱和两类，前者如Sigmoid和tanh，后者如ReLU。使用激活函数可避免网络退化为线性模型，并解决梯度消失等问题，从而增强模型的表达能力。">
  <meta itemprop="datePublished" content="2025-01-30T00:00:00+08:00">
  <meta itemprop="dateModified" content="2025-01-30T00:00:00+08:00">
  <meta itemprop="wordCount" content="181">
  <meta itemprop="keywords" content="机器学习,激活函数,Sigmoid,Softmax">

      <header>
        <h1 itemprop="headline">【深度学习】常见的激活函数</h1>
        <p class="text-sm">
          
            Thursday, Jan 30, 2025
          

          | <span>1 minute read</span>

          
          | <span>Updated at
            
              Thursday, Jan 30, 2025
            </span>
          
        </p>

        
        <div class="flex justify-between">
          
            <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name">古月月仔</span>
  
  </span>
</div>

          

          <div class="flex items-center gap-2">
  
  

  
  
  
  
  
    <a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary"
      href="https://x.com/intent/post?text=%e3%80%90%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e3%80%91%e5%b8%b8%e8%a7%81%e7%9a%84%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0&amp;url=http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener noreferrer"
      title="Share on X">
      <ion-icon class="group-hover:text-primary-content" name="logo-x"></ion-icon>
    </a>
  
    <a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary"
      href="https://facebook.com/sharer/sharer.php?u=http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener noreferrer"
      title="Share on Facebook">
      <ion-icon class="group-hover:text-primary-content" name="logo-facebook"></ion-icon>
    </a>
  
    <a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary"
      href="https://wa.me/?text=%e3%80%90%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e3%80%91%e5%b8%b8%e8%a7%81%e7%9a%84%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0%20http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener noreferrer"
      title="Share on WhatsApp">
      <ion-icon class="group-hover:text-primary-content" name="logo-whatsapp"></ion-icon>
    </a>
  

  
  
</div>

        </div>
      </header>

      <section id="dream-single-post-content" itemprop="articleBody">
        

        <p>激活函数为神经网络引入非线性，使其能够学习复杂模式。它分为饱和与非饱和两类，前者如Sigmoid和tanh，后者如ReLU。使用激活函数可避免网络退化为线性模型，并解决梯度消失等问题，从而增强模型的表达能力。</p>
<p><em><strong>激活函数（Activation Function）</strong></em>,是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/d9bc438935856aa36e7c169fb27dffbd.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<h3 id="什么是激活函数">什么是激活函数</h3>
<p>在神经元中，输入的input经过一系列加权求和后作用于另一个函数，这个函数就是这里的激活函数。类似于人类大脑中基于神经元的模型，激活函数最终决定了是否传递信号以及要发射给下一个神经元的内容。
激活函数为神经网络引入了非线性元素，使得网络能够逼近复杂的非线性函数，从而解决更广泛的问题。</p>
<h3 id="为什么要使用激活函数">为什么要使用激活函数</h3>
<p>如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的<em><strong>感知机（Perceptron）</strong></em>。
使用激活函数能够给神经元引入非线性因素，使得神经网络可以任意逼近任何非线性函数，使深层神经网络表达能力更加强大，这样神经网络就可以应用到众多的非线性模型中。</p>
<h3 id="激活函数的分类">激活函数的分类</h3>
<p>基于激活函数的本质，可以将激活函数分为以下两大类：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image-20250425155707483.png" 
       alt="image-20250425155707483" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      image-20250425155707483
    </figcaption>
  
</figure></p>
<ul>
<li><em><strong>饱和激活函数</strong></em>：Sigmoid、tanh&hellip;&hellip;</li>
<li><em><strong>非饱和激活函数</strong></em>：ReLU、LeakyRelu、ELU、PReLU、RReLU&hellip;.</li>
</ul>
<h4 id="什么是饱和">什么是饱和？</h4>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/image-20250425155722212.png" 
       alt="image-20250425155722212" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      image-20250425155722212
    </figcaption>
  
</figure>
反之，不满足以上条件的函数则称为非饱和激活函数。
<em><strong>Sigmoid</strong></em>函数需要一个实值输入压缩至[0,1]的范围
<em><strong>tanh函数</strong></em>需要讲一个实值输入压缩至 [-1, 1]的范围
相对于饱和激活函数，使用非饱和激活函数的优势在于两点：</p>
<ul>
<li>非饱和激活函数能解决深度神经网络（层数非常多）带来的梯度消失问题</li>
<li>使用非饱和激活函数能加快收敛速度。</li>
</ul>
<h3 id="常见的激活函数">常见的激活函数</h3>
<h4 id="1sigmoid函数">1.Sigmoid函数</h4>
<p>对于二分类问题，<strong>Sigmoid</strong>是一个常用的激活函数，它将任意实数映射到（0，1）区间，这个区间内的数值可以自然地解释为概率
Sigmoid激活函数的数学表达式为:
</p>
$$
f(x)=\frac{1}{1+e^{-x}}
$$<p>其导数的表达式为
</p>
$$
f'(x)=f(x)(1-f(x))
$$<p>函数图像如下：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/6307a876fb9dc6b4bb8f403cb9b5d05e.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<h5 id="什么情况下适合使用sigmoid">什么情况下适合使用Sigmoid？</h5>
<ul>
<li>Sigmoid 函数的输出范围是 0 到 1。非常适合作为模型的输出函数用于输出一个0~1范围内的概率值，比如用于表示二分类的类别或者用于表示置信度。</li>
<li>梯度平滑，便于求导，也防止模型训练过程中出现突变的梯度。</li>
</ul>
<h5 id="sigmoid函数的问题">Sigmoid函数的问题</h5>
<ul>
<li>容易造成梯度消失。我们从导函数图像中了解到sigmoid的导数都是小于0.25的，那么在进行反向传播的时候，梯度相乘结果会慢慢的趋向于0。这样几乎就没有梯度信号通过神经元传递到前面层的梯度更新中，因此这时前面层的权值几乎没有更新，这就叫梯度消失。除此之外，为了防止饱和，必须对于权重矩阵的初始化特别留意。如果初始化权重过大，可能很多神经元得到一个比较小的梯度，致使神经元不能很好的更新权重提前饱和，神经网络就几乎不学习。</li>
<li>函数输出不是以 0 为中心的，梯度可能就会向特定方向移动，从而降低权重更新的效率.</li>
<li>Sigmoid 函数执行指数运算，计算机运行得较慢，比较消耗计算资源。</li>
</ul>
<h4 id="2softmax函数">2.Softmax函数</h4>
<p>Softmax函数又被称为归一化指数函数。它是二分类函数<em><strong>Sigmoid</strong></em>在多分类上的推广，作用是将多分类的结果以概率的形式展现出来。
Softmax激活函数的数学表达式:
</p>
$$
Softmax(x)=\frac{e^{x_i}}{\sum_ie^{x_i}}
$$<p>
函数图像如下：
<figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/1032a2d240e7851977bd6ee3d66d6d6f.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure>
Softmax函数常在神经网络输出层充当激活函数，将输出层的值通过激活函数映射到0-1区间，将神经元输出构造成概率分布，用于多分类问题中，Softmax激活函数映射值越大，则真实类别可能性越大
下图给出了Softmax作激活函数对输出值映射处理过程
<figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/cc6be5ebf4f89f4642d20ecfb64dab15.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<h4 id="3tanh函数">3.Tanh函数</h4>
<p><code>tanh</code>激活函数的数学表达式为:
</p>
$$
f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$<p><code>tanh</code>函数图像如下：
<figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/85f783030561ab608284177ad5e2af1d.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure>
Tanh函数是sigmoid的变形: </p>
$$tanh(x)=2sigmoid(2x)-1$$<p>
与sigmoid不同的是，tanh是“零为中心”的。因此在实际应用中，tanh会比sigmoid更好一些。但是在饱和神经元的情况下，tanh还是没有解决梯度消失问题。</p>
<h5 id="什么情况下适合使用tanh">什么情况下适合使用Tanh？</h5>
<ul>
<li>tanh 的输出间隔为1，并且整个函以0为中心</li>
<li>在tanh图中，负输入将被强映射为负，而零输入被映射为接近0.</li>
</ul>
<h5 id="tanh有哪些缺点">Tanh有哪些缺点？</h5>
<ul>
<li>仍然存在梯度饱和的问题。</li>
<li>依然是指数运算。</li>
</ul>
<h4 id="4relu函数">4.ReLU函数</h4>
<p><em><strong>Relu激活函数</strong></em>的数学表达式为:
</p>
$$
f(x)=max(0,x)
$$<p>函数图像如下：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/4b2b619124c8be86257cc9aeac3b909f.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<h5 id="什么情况下适合使用relu">什么情况下适合使用ReLU?</h5>
<ul>
<li>ReLU解决了梯度消失的问题，当输入值为正时，神经元不会饱和</li>
<li>由于ReLU线性、非饱和的性质，在SGD中能够快速收敛</li>
<li>计算复杂度低，不需要进行指数运算</li>
</ul>
<h5 id="relu有哪些缺点">ReLU有哪些缺点？</h5>
<ul>
<li>与Sigmoid一样，其输出不是以0为中心的</li>
<li>Dead ReLU 问题。当输入为负时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新
训练神经网络的时候，一旦学习率没有设置好，第一次更新权重的时候，输入是负值，那么这个含有ReLU的神经节点就会死亡，再也不会被激活。所以，要设置一个合适的较小的学习率，来降低这种情况的发生。</li>
</ul>
<h4 id="5leaky-relu函数">5.Leaky ReLU函数</h4>
<p><em><strong>Leaky ReLU激活函数</strong></em>的数学表达式为:
</p>
$$
f(x)=max(ax,x)
$$<p>函数图像如下：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/0c53598c1e361283a9bcfad666a86612.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<h5 id="leaky-relu函数的优点">Leaky ReLU函数的优点？</h5>
<ul>
<li>解决了ReLU输入值为负时神经元出现的死亡问题</li>
<li>Leaky ReLU线性、非饱和的性质，在SGD中能够快速收敛</li>
<li>计算复杂度低，不需要进行指数运算</li>
</ul>
<h5 id="leaky-relu的缺点">Leaky ReLU的缺点？</h5>
<ul>
<li>函数中的 $$a$$,需要通过先验知识人工赋值（一般设为0.01）</li>
<li>有些近似线性，导致在复杂分类中的效果不好。
从理论上讲，Leaky ReLU 具有 ReLU 的所有优点，而且 Dead ReLU 不会有任何问题，但在实际操作中，尚未完全证明 Leaky ReLU 总是比 ReLU 更好</li>
</ul>
<h4 id="6prelu函数">6.PRelu函数</h4>
<p>PRelu激活函数的数学表达式为:</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/da48575ecd71596a5b95cca6629671aa.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<p>函数图像如下：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/061dcb658ffc78a999927aac990c10b3.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<p>PRelu激活函数也是用来解决ReLU带来的神经元坏死的问题。与Leaky ReLU激活函数不同的是，PRelu激活函数负半轴的斜率参数α 是通过学习得到的，而不是手动设置的恒定值</p>
<h4 id="7elu函数">7.ELU函数</h4>
<p>ELU激活函数的数学表达式为:</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/cd4db253a23f6b5a4abdb2ebc038e6e6.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure>
函数图像如下：
<figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/0d6517be9a027fce04817cf77d52b631.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<h4 id="8selu函数">8.SELU函数</h4>
<p>SELU激活函数的数学表达式为：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/5818d725fdc176d4f4451609ff32e094.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<p>函数示例图像如下：
<figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/2c05ae1a01cc1f10614962192da99bb1.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<p>SELU 允许构建一个映射 g，其性质能够实现 SNN（自归一化神经网络）。SNN 不能通过ReLU、sigmoid 、tanh 和 Leaky ReLU 实现。这个激活函数需要有：（1）负值和正值，以便控制均值；（2）饱和区域（导数趋近于零），以便抑制更低层中较大的方差；（3）大于 1 的斜率，以便在更低层中的方差过小时增大方差；（4）连续曲线。后者能确保一个固定点，其中方差抑制可通过方差增大来获得均衡。通过乘上指数线性单元（ELU）来满足激活函数的这些性质，而且 λ&gt;1 能够确保正值净输入的斜率大于 1</p>
<p>SELU激活函数是在自归一化网络中定义的，通过调整均值和方差来实现内部的归一化，这种内部归一化比外部归一化更快，这使得网络能够更快得收敛</p>
<h4 id="9swish函数">9.Swish函数</h4>
<p>Swish激活函数的数学表达式为:
</p>
$$
f(x)=x*sigmoid(x)
$$<p>函数图像如下：
<figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/dfe81720562bfb809d77fb516990b1e9.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure>
Swish激活函数无界性有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和；同时，有界性也是有优势的，因为有界激活函数可以具有很强的正则化(防止过拟合， 进而增强泛化能力)，并且较大的负输入问题也能解决
Swish激活函数在x=0附近更为平滑，而非单调的特性增强了输入数据和要学习的权重的表达能力。</p>
<h4 id="10mish函数">10.Mish函数</h4>
<p><code>Mish</code>激活函数的数学表达式为:
</p>
$$
f(x)=x*tanh(ln(1+e^x))
$$<p>函数图像如下：</p>
<p><figure style="text-align: center; margin: 1.5rem auto;">
  
  <img src="http://localhost:1313/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/ec25dd71f7936401034e1d8688461275.png" 
       alt="img" 
       
       class="zoomable" 
       style="max-width: 100%; height: auto; border-radius: 8px; cursor: zoom-in;"
       loading="lazy" />
  
    <figcaption style="margin-top: 8px; font-size: 0.85em; color: #888; font-style: italic;">
      img
    </figcaption>
  
</figure></p>
<p>Mish激活函数的函数图像与Swish激活函数类似，但要更为平滑一些，缺点是计算复杂度要更高一些</p>
<p>博客参考链接</p>
<ul>
<li><a href="https://blog.csdn.net/caip12999203000/article/details/127067360?ops_request_misc=%7B%22request%5Fid%22%3A%22dbe52ff27a31fe27bdcfc2e069793b70%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=dbe52ff27a31fe27bdcfc2e069793b70&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-127067360-null-null.142%5ev102%5epc_search_result_base5&amp;utm_term=%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0&amp;spm=1018.2226.3001.4187" target="_blank">常用的激活函数合集（详细版）-CSDN博客</a>
</li>
<li><a href="https://blog.csdn.net/weixin_39910711/article/details/114849349?ops_request_misc=%7B%22request%5Fid%22%3A%22dbe52ff27a31fe27bdcfc2e069793b70%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=dbe52ff27a31fe27bdcfc2e069793b70&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-114849349-null-null.142%5ev102%5epc_search_result_base5&amp;utm_term=%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0&amp;spm=1018.2226.3001.4187" target="_blank">激活函数（Activation Function）-CSDN博客</a>
</li>
</ul>


        
      </section>

      

      
    </article>
  </div>

  <div
    x-data="tocHighlighter()"
    @scroll.window="debouncedScroll"
    class="hidden lg:flex lg:flex-col lg:items-end lg:self-start"
  >
    
  </div>
</div>


            
<footer class="flex justify-between items-center gap-2 px-4 py-12">

  <div>
  
  <p>
    © 2021 - 2026 古月月仔的博客
  </p>
  

  
  <p class="text-sm">
    🌱
    <span class="text-base-content/60">
      Powered by <a class="hover:underline" href="https://gohugo.io/" target="_blank">Hugo</a> with theme
      <a class="hover:underline" href="https://github.com/g1eny0ung/hugo-theme-dream" target="_blank">Dream</a>.</span
    >
  </p>
  
</div>

  <div
  x-data="{ icons: [
    { name: 'sunny', status: 'n' },
    { name: 'moon', status: 'y' },
    { name: 'desktop', status: 'auto' }
  ] }"
  class="flex items-center gap-2 h-[32px] px-2 bg-base-100 border border-base-content/30 rounded-full"
>
  <template x-for="icon in icons">
    <div
      role="button"
      tabindex="0"
      :aria-label="'Select ' + icon.name + ' mode'"
      class="group inline-flex justify-center items-center p-1 rounded-full cursor-pointer hover:bg-primary"
      :class="$store.darkMode.icon() === icon.name && 'bg-primary'"
      @click="$store.darkMode.toggle(icon.status)"
    >
      <ion-icon
        :name="`${icon.name}-outline`"
        class="group-hover:text-primary-content"
        :class="$store.darkMode.icon() === icon.name && 'text-primary-content'"
      >
      </ion-icon>
    </div>
  </template>
</div>

</footer>

          </div>
        </div>
        <div class="back">
          <div class="container">
            
            <div class="dream-grid dream-grid-about">
  
  
  
  <div class="w-full md:w-1/2 lg:w-1/3 xl:w-1/4 p-4 dream-column">
    <article
      class="card bg-base-100 hover:bg-base-content/10 shadow-xl dark:border dark:border-base-content/30"
    >
      <div class="card-body">
        <div class="card-title">关于我</div>

        <div class="prose dark:prose-invert">
          <ul>
<li>我是<strong>古月月仔</strong></li>
<li><strong>Ethan Hu</strong></li>
<li>分享技术学习笔记与生活点滴</li>
<li><strong>现居</strong>： 上海 中国</li>
<li><strong>家乡</strong>： 平遥 山西</li>
</ul>

        </div>
      </div>
    </article>
  </div>
  
  <div class="w-full md:w-1/2 lg:w-1/3 xl:w-1/4 p-4 dream-column">
    <article
      class="card bg-base-100 hover:bg-base-content/10 shadow-xl dark:border dark:border-base-content/30"
    >
      <div class="card-body">
        <div class="card-title">在用的学习工具</div>

        <div class="prose dark:prose-invert">
          <ul>
<li>📝 <a href="https://typora.io/" target="_blank">Typora</a>
 —— 极致简洁的 Markdown 编辑器，助力沉浸式文档撰写与知识记录。</li>
<li>📓 <a href="https://www.notion.so/" target="_blank">Notion</a>
 —— 一站式工作空间，用于搭建个人知识库、项目管理与深度协作。</li>
<li>🔗 <a href="https://n8n.io/" target="_blank">N8N</a>
 —— 强大的基于节点的自动化工作流工具，轻松实现不同应用间的逻辑联动。</li>
<li>🤖 <a href="https://gemini.google.com/" target="_blank">Gemini</a>
 —— 智能 AI 助手，在代码辅助、创意激发与信息检索中提供强力支撑。</li>
</ul>

        </div>
      </div>
    </article>
  </div>
  
  <div class="w-full md:w-1/2 lg:w-1/3 xl:w-1/4 p-4 dream-column">
    <article
      class="card bg-base-100 hover:bg-base-content/10 shadow-xl dark:border dark:border-base-content/30"
    >
      <div class="card-body">
        <div class="card-title">我的爱好</div>

        <div class="prose dark:prose-invert">
          <ul>
<li>🚀 喜欢折腾各种好玩的技术</li>
<li>📸 业余摄影爱好者</li>
<li>🎮 各类游戏玩家</li>
<li>💻 数码产品折腾爱好者</li>
<li>📚 阅读：赫尔曼·黑塞 &amp; 阿尔贝·加缪</li>
<li>🎞️ 追番中：《电锯人：蕾塞篇》</li>
<li>🎬 经典重温：《命运石之门》</li>
</ul>

        </div>
      </div>
    </article>
  </div>
  
  <div class="w-full md:w-1/2 lg:w-1/3 xl:w-1/4 p-4 dream-column">
    <article
      class="card bg-base-100 hover:bg-base-content/10 shadow-xl dark:border dark:border-base-content/30"
    >
      <div class="card-body">
        <div class="card-title">最近正在学</div>

        <div class="prose dark:prose-invert">
          <ul>
<li>🎨 <a href="https://games104.boomingtech.com/" target="_blank">GAMES104</a>
 —— 现代游戏引擎入门。</li>
<li>🏔️ <a href="https://www.sidefx.com/learn/" target="_blank">Houdini 基础</a>
 —— 学习程序化建模与特效。</li>
<li>🤗 <a href="https://huggingface.co/learn" target="_blank">Hugging Face</a>
 —— 探索开源 AI 社区。</li>
</ul>

        </div>
      </div>
    </article>
  </div>
  
  

  

  
</div>

            

            
<footer class="flex justify-between items-center gap-2 px-4 py-12">

  <div>
  
  <p>
    © 2021 - 2026 古月月仔的博客
  </p>
  

  
  <p class="text-sm">
    🌱
    <span class="text-base-content/60">
      Powered by <a class="hover:underline" href="https://gohugo.io/" target="_blank">Hugo</a> with theme
      <a class="hover:underline" href="https://github.com/g1eny0ung/hugo-theme-dream" target="_blank">Dream</a>.</span
    >
  </p>
  
</div>

  <div
  x-data="{ icons: [
    { name: 'sunny', status: 'n' },
    { name: 'moon', status: 'y' },
    { name: 'desktop', status: 'auto' }
  ] }"
  class="flex items-center gap-2 h-[32px] px-2 bg-base-100 border border-base-content/30 rounded-full"
>
  <template x-for="icon in icons">
    <div
      role="button"
      tabindex="0"
      :aria-label="'Select ' + icon.name + ' mode'"
      class="group inline-flex justify-center items-center p-1 rounded-full cursor-pointer hover:bg-primary"
      :class="$store.darkMode.icon() === icon.name && 'bg-primary'"
      @click="$store.darkMode.toggle(icon.status)"
    >
      <ion-icon
        :name="`${icon.name}-outline`"
        class="group-hover:text-primary-content"
        :class="$store.darkMode.icon() === icon.name && 'text-primary-content'"
      >
      </ion-icon>
    </div>
  </template>
</div>

</footer>

          </div>
        </div>
      </div>
    </div>

    <script>
  window.lightTheme = "emerald"
  window.darkTheme = "forest"
</script>


  <script src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js"></script>

  
  
  <script src="/js/grid.js"></script>




<script src="/js/main.js"></script>
    







<script src="/js/toc.js"></script>




  




    

    
      <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [
        ['\\[', '\\]'],
        ['$$', '$$'],
      ], 
      inlineMath: [
        ['\\(', '\\)'],
        ['$', '$'],
      ], 
    },
  }
</script>

    

    

    

    <script type="module" src="https://cdn.jsdelivr.net/npm/ionicons@7.4.0/dist/ionicons/ionicons.esm.js" integrity="sha256-/IFmi82bIhdYWctu0UddSlJqpnzWm7Vh2C4CM32wF/k=" crossorigin="anonymous"></script>
    <script nomodule src="https://cdn.jsdelivr.net/npm/ionicons@7.4.0/dist/ionicons/ionicons.js" integrity="sha256-mr7eJMX3VC3F7G32mk4oWp1C6a2tlMYxUdptfT7uKI8=" crossorigin="anonymous"></script>
  </body>
</html>
